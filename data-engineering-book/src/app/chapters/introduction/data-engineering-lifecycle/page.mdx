# The
import {MermaidDiagram} from '@/components/MermaidDiagram';

# Data Engineering Lifecycle

The data engineering lifecycle represents the end-to-end process of moving data from source systems to analytics-ready formats. Understanding this lifecycle is crucial for designing effective data systems and managing data projects successfully.

## Lifecycle Overview

<MermaidDiagram chart={`
graph LR
    A[Generation] --> B[Storage]
    B --> C[Ingestion]
    C --> D[Transformation]
    D --> E[Serving]
    E --> F[Analytics]
    
    subgraph "Undercurrents"
        G[Security]
        H[Data Management]
        I[DataOps]
        J[Data Architecture]
        K[Orchestration]
        L[Software Engineering]
    end
    
    G -.-> A
    G -.-> B
    G -.-> C
    G -.-> D
    G -.-> E
    G -.-> F
    
    style A fill:#e3f2fd
    style F fill:#f3e5f5
    style G fill:#fff3e0
`} />

## Stage 1: Generation

Data generation is the starting point where data is created in source systems.

### Common Data Sources

```python
# Example: Monitoring different data sources
class DataSourceMonitor:
    def __init__(self):
        self.sources = {
            'transactional_db': {
                'type': 'PostgreSQL',
                'tables': ['users', 'orders', 'products'],
                'update_frequency': 'real-time'
            },
            'web_analytics': {
                'type': 'Event Stream',
                'events': ['page_view', 'click', 'purchase'],
                'volume': '10M events/day'
            },
            'external_api': {
                'type': 'REST API',
                'endpoints': ['/customers', '/inventory'],
                'rate_limit': '1000 req/min'
            }
        }
    
    def check_source_health(self, source_name):
        """Monitor data source availability and quality"""
        source = self.sources[source_name]
        
        if source['type'] == 'PostgreSQL':
            return self.check_database_health(source)
        elif source['type'] == 'Event Stream':
            return self.check_stream_health(source)
        elif source['type'] == 'REST API':
            return self.check_api_health(source)
    
    def check_database_health(self, source):
        # Check connection, query performance, data freshness
        return {
            'status': 'healthy',
            'last_update': '2024-01-15 10:30:00',
            'row_count': 1500000,
            'avg_query_time': '45ms'
        }
```

### Data Generation Patterns

- **Transactional Systems**: OLTP databases (PostgreSQL, MySQL)
- **Event Streams**: User interactions, IoT sensors
- **Batch Files**: Daily exports, log files
- **APIs**: Third-party services, microservices

## Stage 2: Storage

Raw data storage provides the foundation for all downstream processing.

### Storage Technologies

```python
# Example: Multi-tier storage strategy
class StorageManager:
    def __init__(self):
        self.storage_tiers = {
            'hot': {
                'technology': 'SSD',
                'access_pattern': 'frequent',
                'retention': '30 days',
                'cost_per_gb': 0.10
            },
            'warm': {
                'technology': 'Standard Storage',
                'access_pattern': 'occasional',
                'retention': '1 year',
                'cost_per_gb': 0.02
            },
            'cold': {
                'technology': 'Archive Storage',
                'access_pattern': 'rare',
                'retention': '7 years',
                'cost_per_gb': 0.004
            }
        }
    
    def determine_storage_tier(self, data_metadata):
        """Determine appropriate storage tier based on access patterns"""
        access_frequency = data_metadata['access_frequency']
        data_age = data_metadata['age_days']
        
        if access_frequency > 100 and data_age < 30:
            return 'hot'
        elif access_frequency > 10 and data_age < 365:
            return 'warm'
        else:
            return 'cold'
    
    def calculate_storage_cost(self, data_size_gb, tier):
        """Calculate monthly storage cost"""
        tier_info = self.storage_tiers[tier]
        return data_size_gb * tier_info['cost_per_gb']
```

### Storage Considerations

- **Durability**: Data replication and backup strategies
- **Availability**: Uptime requirements and disaster recovery
- **Scalability**: Ability to handle growing data volumes
- **Cost**: Balance between performance and expense

## Stage 3: Ingestion

Data ingestion moves data from source systems to processing environments.

### Ingestion Patterns

<MermaidDiagram chart={`
graph TB
    subgraph "Batch Ingestion"
        A[Source DB] --> B[ETL Job]
        B --> C[Data Warehouse]
    end
    
    subgraph "Stream Ingestion"
        D[Event Source] --> E[Message Queue]
        E --> F[Stream Processor]
        F --> G[Real-time Store]
    end
    
    subgraph "Micro-batch"
        H[Source API] --> I[Mini-batch Job]
        I --> J[Processing Queue]
        J --> K[Target System]
    end
    
    style A fill:#e3f2fd
    style D fill:#e8f5e8
    style H fill:#fff3e0
`} />

### Batch Ingestion Example

```python
# Example: Robust batch ingestion pipeline
import pandas as pd
from datetime import datetime, timedelta
import logging

class BatchIngestionPipeline:
    def __init__(self, source_config, target_config):
        self.source = source_config
        self.target = target_config
        self.logger = logging.getLogger(__name__)
    
    def extract_incremental_data(self, table_name, last_processed_timestamp):
        """Extract only new/updated records since last run"""
        query = f"""
        SELECT * FROM {table_name}
        WHERE updated_at > '{last_processed_timestamp}'
        ORDER BY updated_at
        """
        
        try:
            df = pd.read_sql(query, self.source['connection'])
            self.logger.info(f"Extracted {len(df)} records from {table_name}")
            return df
        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")
            raise
    
    def validate_data_quality(self, df, table_name):
        """Validate data quality before processing"""
        quality_checks = {
            'null_check': df.isnull().sum().sum() == 0,
            'duplicate_check': df.duplicated().sum() == 0,
            'schema_check': self.validate_schema(df, table_name)
        }
        
        failed_checks = [check for check, passed in quality_checks.items() if not passed]
        
        if failed_checks:
            self.logger.warning(f"Quality checks failed: {failed_checks}")
            return False
        return True
    
    def run_ingestion(self, table_name):
        """Execute the complete ingestion process"""
        try:
            # Get last processed timestamp
            last_timestamp = self.get_last_processed_timestamp(table_name)
            
            # Extract data
            df = self.extract_incremental_data(table_name, last_timestamp)
            
            if df.empty:
                self.logger.info(f"No new data for {table_name}")
                return
            
            # Validate data quality
            if not self.validate_data_quality(df, table_name):
                raise ValueError("Data quality validation failed")
            
            # Load to target
            self.load_data(df, table_name)
            
            # Update watermark
            self.update_last_processed_timestamp(table_name, df['updated_at'].max())
            
        except Exception as e:
            self.logger.error(f"Ingestion failed for {table_name}: {e}")
            raise
```

### Stream Ingestion Example

```python
# Example: Real-time stream processing with Kafka
from kafka import KafkaConsumer, KafkaProducer
import json
from datetime import datetime

class StreamProcessor:
    def __init__(self, kafka_config):
        self.consumer = KafkaConsumer(
            'user-events',
            bootstrap_servers=kafka_config['servers'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_config['servers'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
    
    def process_user_event(self, event):
        """Process individual user event"""
        # Enrich event with additional data
        enriched_event = {
            **event,
            'processed_at': datetime.utcnow().isoformat(),
            'user_segment': self.get_user_segment(event['user_id']),
            'session_duration': self.calculate_session_duration(event)
        }
        
        # Apply business rules
        if self.is_suspicious_activity(enriched_event):
            self.send_alert(enriched_event)
        
        return enriched_event
    
    def run_stream_processing(self):
        """Main stream processing loop"""
        for message in self.consumer:
            try:
                event = message.value
                processed_event = self.process_user_event(event)
                
                # Send to downstream topics
                self.producer.send('processed-events', processed_event)
                
                # Update real-time aggregations
                self.update_real_time_metrics(processed_event)
                
            except Exception as e:
                print(f"Error processing event: {e}")
                # Send to dead letter queue
                self.producer.send('failed-events', message.value)
```

## Stage 4: Transformation

Data transformation applies business logic and prepares data for analytics.

### Transformation Types

```python
# Example: Comprehensive data transformation pipeline
class DataTransformer:
    def __init__(self):
        self.transformations = []
    
    def clean_data(self, df):
        """Basic data cleaning operations"""
        # Remove duplicates
        df = df.drop_duplicates()
        
        # Handle missing values
        numeric_columns = df.select_dtypes(include=['number']).columns
        df[numeric_columns] = df[numeric_columns].fillna(0)
        
        categorical_columns = df.select_dtypes(include=['object']).columns
        df[categorical_columns] = df[categorical_columns].fillna('Unknown')
        
        return df
    
    def apply_business_rules(self, df):
        """Apply domain-specific business logic"""
        # Calculate customer lifetime value
        df['clv'] = df['total_purchases'] * df['avg_order_value']
        
        # Categorize customers
        df['customer_tier'] = pd.cut(
            df['clv'],
            bins=[0, 1000, 5000, float('inf')],
            labels=['Bronze', 'Silver', 'Gold']
        )
        
        # Flag high-risk transactions
        df['high_risk'] = (
            (df['transaction_amount'] > df['avg_transaction'] * 3) |
            (df['transaction_hour'].isin([0, 1, 2, 3, 4, 5]))
        )
        
        return df
    
    def aggregate_data(self, df, group_by_columns, agg_functions):
        """Create aggregated views of the data"""
        return df.groupby(group_by_columns).agg(agg_functions).reset_index()
    
    def transform_pipeline(self, df):
        """Execute complete transformation pipeline"""
        # Step 1: Clean data
        df = self.clean_data(df)
        
        # Step 2: Apply business rules
        df = self.apply_business_rules(df)
        
        # Step 3: Create aggregations
        daily_summary = self.aggregate_data(
            df,
            ['date', 'customer_tier'],
            {
                'transaction_amount': ['sum', 'mean', 'count'],
                'clv': 'mean'
            }
        )
        
        return df, daily_summary
```

## Stage 5: Serving

Data serving makes processed data available for consumption by downstream applications.

### Serving Patterns

<MermaidDiagram chart={`
graph LR
    A[Processed Data] --> B{Serving Layer}
    B --> C[OLAP Queries]
    B --> D[ML Models]
    B --> E[Dashboards]
    B --> F[APIs]
    
    C --> G[Data Warehouse]
    D --> H[Feature Store]
    E --> I[BI Tools]
    F --> J[Applications]
    
    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style G fill:#f3e5f5
    style H fill:#f3e5f5
    style I fill:#f3e5f5
    style J fill:#f3e5f5
`} />

### Serving Implementation

```python
# Example: Multi-purpose data serving layer
class DataServingLayer:
    def __init__(self, connections):
        self.warehouse = connections['warehouse']
        self.cache = connections['redis']
        self.feature_store = connections['feature_store']
    
    def serve_analytical_query(self, query):
        """Serve complex analytical queries"""
        # Check cache first
        cache_key = f"query:{hash(query)}"
        cached_result = self.cache.get(cache_key)
        
        if cached_result:
            return json.loads(cached_result)
        
        # Execute query against warehouse
        result = pd.read_sql(query, self.warehouse)
        
        # Cache result for future requests
        self.cache.setex(
            cache_key,
            3600,  # 1 hour TTL
            result.to_json()
        )
        
        return result
    
    def serve_ml_features(self, entity_ids, feature_names):
        """Serve features for ML model inference"""
        features = {}
        
        for feature_name in feature_names:
            # Get features from feature store
            feature_values = self.feature_store.get_online_features(
                feature_view=feature_name,
                entity_rows=[{"entity_id": eid} for eid in entity_ids]
            )
            features[feature_name] = feature_values
        
        return features
    
    def serve_real_time_metrics(self, metric_name, time_window):
        """Serve real-time aggregated metrics"""
        # Get from real-time aggregation store
        return self.cache.get(f"metric:{metric_name}:{time_window}")
```

## Undercurrents: Cross-Cutting Concerns

### Security

```python
# Example: Data security implementation
class DataSecurity:
    def __init__(self):
        self.encryption_key = self.load_encryption_key()
    
    def encrypt_pii(self, df, pii_columns):
        """Encrypt personally identifiable information"""
        for column in pii_columns:
            if column in df.columns:
                df[column] = df[column].apply(self.encrypt_value)
        return df
    
    def apply_data_masking(self, df, sensitive_columns):
        """Apply data masking for non-production environments"""
        for column in sensitive_columns:
            if column in df.columns:
                df[column] = df[column].apply(self.mask_value)
        return df
    
    def audit_data_access(self, user_id, table_name, query):
        """Log data access for compliance"""
        audit_record = {
            'timestamp': datetime.utcnow(),
            'user_id': user_id,
            'table_name': table_name,
            'query_hash': hash(query),
            'access_type': 'read'
        }
        self.log_audit_event(audit_record)
```

### Data Quality Monitoring

```python
# Example: Comprehensive data quality monitoring
class DataQualityMonitor:
    def __init__(self):
        self.quality_rules = self.load_quality_rules()
        self.alert_thresholds = self.load_alert_thresholds()
    
    def run_quality_checks(self, df, table_name):
        """Run all quality checks for a dataset"""
        results = {}
        
        # Completeness checks
        results['completeness'] = self.check_completeness(df)
        
        # Uniqueness checks
        results['uniqueness'] = self.check_uniqueness(df, table_name)
        
        # Validity checks
        results['validity'] = self.check_validity(df, table_name)
        
        # Consistency checks
        results['consistency'] = self.check_consistency(df)
        
        # Timeliness checks
        results['timeliness'] = self.check_timeliness(df)
        
        return results
    
    def generate_quality_report(self, quality_results):
        """Generate quality report and alerts"""
        report = {
            'overall_score': self.calculate_quality_score(quality_results),
            'failed_checks': [],
            'warnings': []
        }
        
        for check_type, results in quality_results.items():
            if results['score'] < self.alert_thresholds[check_type]:
                report['failed_checks'].append({
                    'type': check_type,
                    'score': results['score'],
                    'details': results['details']
                })
        
        return report
```

## Best Practices

### Pipeline Design Principles

1. **Idempotency**: Pipelines should produce the same result when run multiple times
2. **Monitoring**: Comprehensive logging and alerting
3. **Error Handling**: Graceful failure and recovery mechanisms
4. **Testing**: Unit tests, integration tests, and data quality tests
5. **Documentation**: Clear documentation of data lineage and transformations

### Performance Optimization

```python
# Example: Pipeline performance optimization
class PipelineOptimizer:
    def __init__(self):
        self.performance_metrics = {}
    
    def optimize_query_performance(self, query):
        """Optimize SQL query performance"""
        # Add appropriate indexes
        # Partition large tables
        # Use query hints where necessary
        pass
    
    def implement_caching_strategy(self, pipeline_stage):
        """Implement intelligent caching"""
        # Cache frequently accessed data
        # Use appropriate cache TTL
        # Implement cache invalidation
        pass
    
    def parallelize_processing(self, data_chunks):
        """Parallelize data processing tasks"""
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(self.process_chunk, data_chunks))
        
        return results
```

## Summary

The data engineering lifecycle provides a framework for understanding how data flows through an organization. Each stage has specific challenges and considerations:

- **Generation**: Understanding source systems and data patterns
- **Storage**: Balancing cost, performance, and durability
- **Ingestion**: Choosing between batch and stream processing
- **Transformation**: Applying business logic and ensuring quality
- **Serving**: Optimizing for different consumption patterns

Success in data engineering requires mastering each stage while maintaining focus on the cross-cutting concerns of security, quality, and operational excellence.

---

**Next**: Learn about the [Modern Data Stack](/chapters/introduction/modern-data-stack) and how contemporary tools and technologies support the data engineering lifecycle.
