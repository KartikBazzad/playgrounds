import {MermaidDiagram} from '@/components/MermaidDiagram';

# DataOps Principles

DataOps is a collaborative data management practice focused on improving the communication, integration, and automation of data flows between data managers and data consumers across an organization. It applies DevOps principles to data analytics and engineering workflows.

## DataOps Architecture and Workflow

<MermaidDiagram chart={`
graph TB
    subgraph "Development Phase"
        DP1[Data Source Integration]
        DP2[Pipeline Development]
        DP3[Data Transformation]
        DP4[Quality Validation]
    end
    
    subgraph "Testing & Validation"
        TV1[Unit Testing]
        TV2[Integration Testing]
        TV3[Data Quality Tests]
        TV4[Performance Testing]
    end
    
    subgraph "Deployment Pipeline"
        DPL1[Version Control]
        DPL2[Automated Build]
        DPL3[Environment Promotion]
        DPL4[Production Deployment]
    end
    
    subgraph "Monitoring & Operations"
        MO1[Pipeline Monitoring]
        MO2[Data Quality Monitoring]
        MO3[Performance Metrics]
        MO4[Incident Response]
    end
    
    subgraph "Feedback Loop"
        FL1[Usage Analytics]
        FL2[Business Feedback]
        FL3[Performance Insights]
        FL4[Continuous Improvement]
    end
    
    DP1 --> TV1
    DP2 --> TV2
    DP3 --> TV3
    DP4 --> TV4
    
    TV1 --> DPL1
    TV2 --> DPL2
    TV3 --> DPL3
    TV4 --> DPL4
    
    DPL1 --> MO1
    DPL2 --> MO2
    DPL3 --> MO3
    DPL4 --> MO4
    
    MO1 --> FL1
    MO2 --> FL2
    MO3 --> FL3
    MO4 --> FL4
    
    FL1 --> DP1
    FL2 --> DP2
    FL3 --> DP3
    FL4 --> DP4
    
    style DP1 fill:#e3f2fd
    style TV1 fill:#e8f5e8
    style DPL1 fill:#fff3e0
    style MO1 fill:#f3e5f5
    style FL1 fill:#fce4ec
`} />

## Core DataOps Framework

### DataOps Orchestration System

```python
# Comprehensive DataOps Framework
import os
import yaml
import json
import logging
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import git
import docker
from abc import ABC, abstractmethod

class PipelineStatus(Enum):
    """Pipeline execution status"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"

class EnvironmentType(Enum):
    """Environment types"""
    DEVELOPMENT = "development"
    TESTING = "testing"
    STAGING = "staging"
    PRODUCTION = "production"

@dataclass
class DataOpsConfig:
    """DataOps configuration"""
    project_name: str
    version: str
    environments: Dict[str, Dict[str, Any]]
    pipelines: Dict[str, Dict[str, Any]]
    quality_gates: Dict[str, Any]
    monitoring: Dict[str, Any]
    notifications: Dict[str, Any]
    
    @classmethod
    def from_yaml(cls, config_path: str) -> 'DataOpsConfig':
        """Load configuration from YAML file"""
        with open(config_path, 'r') as f:
            config_data = yaml.safe_load(f)
        
        return cls(
            project_name=config_data['project']['name'],
            version=config_data['project']['version'],
            environments=config_data.get('environments', {}),
            pipelines=config_data.get('pipelines', {}),
            quality_gates=config_data.get('quality_gates', {}),
            monitoring=config_data.get('monitoring', {}),
            notifications=config_data.get('notifications', {})
        )

@dataclass
class PipelineExecution:
    """Pipeline execution record"""
    execution_id: str
    pipeline_name: str
    environment: EnvironmentType
    status: PipelineStatus
    started_at: datetime
    completed_at: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    commit_hash: Optional[str] = None
    triggered_by: Optional[str] = None
    logs: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    artifacts: List[str] = field(default_factory=list)
    
    def complete(self, status: PipelineStatus):
        """Mark pipeline execution as complete"""
        self.completed_at = datetime.now()
        self.status = status
        if self.started_at:
            self.duration_seconds = (self.completed_at - self.started_at).total_seconds()
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'execution_id': self.execution_id,
            'pipeline_name': self.pipeline_name,
            'environment': self.environment.value,
            'status': self.status.value,
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'duration_seconds': self.duration_seconds,
            'commit_hash': self.commit_hash,
            'triggered_by': self.triggered_by,
            'logs_count': len(self.logs),
            'metrics': self.metrics,
            'artifacts_count': len(self.artifacts)
        }

class QualityGate(ABC):
    """Abstract base class for quality gates"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    @abstractmethod
    def evaluate(self, execution: PipelineExecution) -> bool:
        """Evaluate quality gate"""
        pass
    
    @abstractmethod
    def get_metrics(self) -> Dict[str, Any]:
        """Get quality gate metrics"""
        pass

class DataQualityGate(QualityGate):
    """Data quality gate implementation"""
    
    def evaluate(self, execution: PipelineExecution) -> bool:
        """Evaluate data quality metrics"""
        try:
            # Check data quality metrics from execution
            quality_metrics = execution.metrics.get('data_quality', {})
            
            # Define quality thresholds
            completeness_threshold = self.config.get('completeness_threshold', 0.95)
            accuracy_threshold = self.config.get('accuracy_threshold', 0.98)
            consistency_threshold = self.config.get('consistency_threshold', 0.95)
            
            # Evaluate thresholds
            completeness = quality_metrics.get('completeness', 0)
            accuracy = quality_metrics.get('accuracy', 0)
            consistency = quality_metrics.get('consistency', 0)
            
            passed = (
                completeness >= completeness_threshold and
                accuracy >= accuracy_threshold and
                consistency >= consistency_threshold
            )
            
            self.logger.info(f"Data quality gate {self.name}: {'PASSED' if passed else 'FAILED'}")
            self.logger.info(f"  Completeness: {completeness:.3f} (threshold: {completeness_threshold})")
            self.logger.info(f"  Accuracy: {accuracy:.3f} (threshold: {accuracy_threshold})")
            self.logger.info(f"  Consistency: {consistency:.3f} (threshold: {consistency_threshold})")
            
            return passed
            
        except Exception as e:
            self.logger.error(f"Error evaluating data quality gate: {e}")
            return False
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get data quality gate metrics"""
        return {
            'gate_type': 'data_quality',
            'thresholds': {
                'completeness': self.config.get('completeness_threshold', 0.95),
                'accuracy': self.config.get('accuracy_threshold', 0.98),
                'consistency': self.config.get('consistency_threshold', 0.95)
            }
        }

class PerformanceQualityGate(QualityGate):
    """Performance quality gate implementation"""
    
    def evaluate(self, execution: PipelineExecution) -> bool:
        """Evaluate performance metrics"""
        try:
            # Check performance metrics
            performance_metrics = execution.metrics.get('performance', {})
            
            # Define performance thresholds
            max_duration = self.config.get('max_duration_minutes', 60)
            max_memory_mb = self.config.get('max_memory_mb', 2048)
            min_throughput = self.config.get('min_throughput_records_per_second', 100)
            
            # Evaluate thresholds
            duration_minutes = execution.duration_seconds / 60 if execution.duration_seconds else 0
            memory_mb = performance_metrics.get('peak_memory_mb', 0)
            throughput = performance_metrics.get('throughput_records_per_second', 0)
            
            passed = (
                duration_minutes <= max_duration and
                memory_mb <= max_memory_mb and
                throughput >= min_throughput
            )
            
            self.logger.info(f"Performance quality gate {self.name}: {'PASSED' if passed else 'FAILED'}")
            self.logger.info(f"  Duration: {duration_minutes:.1f}min (max: {max_duration}min)")
            self.logger.info(f"  Memory: {memory_mb:.1f}MB (max: {max_memory_mb}MB)")
            self.logger.info(f"  Throughput: {throughput:.1f} rec/s (min: {min_throughput} rec/s)")
            
            return passed
            
        except Exception as e:
            self.logger.error(f"Error evaluating performance quality gate: {e}")
            return False
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get performance quality gate metrics"""
        return {
            'gate_type': 'performance',
            'thresholds': {
                'max_duration_minutes': self.config.get('max_duration_minutes', 60),
                'max_memory_mb': self.config.get('max_memory_mb', 2048),
                'min_throughput_records_per_second': self.config.get('min_throughput_records_per_second', 100)
            }
        }

class DataOpsOrchestrator:
    """Main DataOps orchestration engine"""
    
    def __init__(self, config: DataOpsConfig):
        self.config = config
        self.executions: Dict[str, PipelineExecution] = {}
        self.quality_gates: Dict[str, QualityGate] = {}
        self.logger = logging.getLogger(__name__)
        
        # Initialize quality gates
        self._initialize_quality_gates()
    
    def _initialize_quality_gates(self):
        """Initialize quality gates from configuration"""
        for gate_name, gate_config in self.config.quality_gates.items():
            gate_type = gate_config.get('type')
            
            if gate_type == 'data_quality':
                self.quality_gates[gate_name] = DataQualityGate(gate_name, gate_config)
            elif gate_type == 'performance':
                self.quality_gates[gate_name] = PerformanceQualityGate(gate_name, gate_config)
            else:
                self.logger.warning(f"Unknown quality gate type: {gate_type}")
    
    def execute_pipeline(self, pipeline_name: str, environment: EnvironmentType,
                        triggered_by: str = None, commit_hash: str = None) -> str:
        """Execute a data pipeline"""
        
        # Create execution record
        execution_id = f"{pipeline_name}_{environment.value}_{int(datetime.now().timestamp())}"
        
        execution = PipelineExecution(
            execution_id=execution_id,
            pipeline_name=pipeline_name,
            environment=environment,
            status=PipelineStatus.PENDING,
            started_at=datetime.now(),
            triggered_by=triggered_by,
            commit_hash=commit_hash
        )
        
        self.executions[execution_id] = execution
        
        try:
            # Update status to running
            execution.status = PipelineStatus.RUNNING
            self.logger.info(f"Starting pipeline execution: {execution_id}")
            
            # Get pipeline configuration
            pipeline_config = self.config.pipelines.get(pipeline_name, {})
            if not pipeline_config:
                raise ValueError(f"Pipeline configuration not found: {pipeline_name}")
            
            # Execute pipeline steps
            self._execute_pipeline_steps(execution, pipeline_config)
            
            # Evaluate quality gates
            quality_passed = self._evaluate_quality_gates(execution)
            
            if quality_passed:
                execution.complete(PipelineStatus.SUCCESS)
                self.logger.info(f"Pipeline execution completed successfully: {execution_id}")
            else:
                execution.complete(PipelineStatus.FAILED)
                self.logger.error(f"Pipeline execution failed quality gates: {execution_id}")
            
        except Exception as e:
            execution.complete(PipelineStatus.FAILED)
            execution.logs.append(f"Pipeline execution failed: {str(e)}")
            self.logger.error(f"Pipeline execution failed: {execution_id} - {e}")
        
        return execution_id
    
    def _execute_pipeline_steps(self, execution: PipelineExecution, pipeline_config: Dict[str, Any]):
        """Execute individual pipeline steps"""
        steps = pipeline_config.get('steps', [])
        
        for step_config in steps:
            step_name = step_config.get('name', 'unnamed_step')
            step_type = step_config.get('type')
            step_command = step_config.get('command')
            
            execution.logs.append(f"Executing step: {step_name}")
            self.logger.info(f"Executing step: {step_name} (type: {step_type})")
            
            try:
                if step_type == 'shell':
                    self._execute_shell_step(execution, step_command)
                elif step_type == 'python':
                    self._execute_python_step(execution, step_config)
                elif step_type == 'docker':
                    self._execute_docker_step(execution, step_config)
                else:
                    raise ValueError(f"Unknown step type: {step_type}")
                
                execution.logs.append(f"Step completed: {step_name}")
                
            except Exception as e:
                execution.logs.append(f"Step failed: {step_name} - {str(e)}")
                raise
    
    def _execute_shell_step(self, execution: PipelineExecution, command: str):
        """Execute shell command step"""
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=3600  # 1 hour timeout
            )
            
            if result.returncode != 0:
                raise subprocess.CalledProcessError(result.returncode, command, result.stderr)
            
            execution.logs.append(f"Shell command output: {result.stdout}")
            
        except subprocess.TimeoutExpired:
            raise Exception("Shell command timed out")
    
    def _execute_python_step(self, execution: PipelineExecution, step_config: Dict[str, Any]):
        """Execute Python script step"""
        script_path = step_config.get('script')
        args = step_config.get('args', [])
        
        if not script_path:
            raise ValueError("Python step requires 'script' parameter")
        
        command = ['python', script_path] + args
        
        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=3600
            )
            
            if result.returncode != 0:
                raise subprocess.CalledProcessError(result.returncode, ' '.join(command), result.stderr)
            
            execution.logs.append(f"Python script output: {result.stdout}")
            
        except subprocess.TimeoutExpired:
            raise Exception("Python script timed out")
    
    def _execute_docker_step(self, execution: PipelineExecution, step_config: Dict[str, Any]):
        """Execute Docker container step"""
        image = step_config.get('image')
        command = step_config.get('command')
        volumes = step_config.get('volumes', {})
        environment = step_config.get('environment', {})
        
        if not image:
            raise ValueError("Docker step requires 'image' parameter")
        
        try:
            client = docker.from_env()
            
            container = client.containers.run(
                image=image,
                command=command,
                volumes=volumes,
                environment=environment,
                detach=True,
                remove=True
            )
            
            # Wait for container to complete
            result = container.wait()
            logs = container.logs().decode('utf-8')
            
            if result['StatusCode'] != 0:
                raise Exception(f"Docker container failed with status {result['StatusCode']}")
            
            execution.logs.append(f"Docker container output: {logs}")
            
        except docker.errors.DockerException as e:
            raise Exception(f"Docker execution failed: {str(e)}")
    
    def _evaluate_quality_gates(self, execution: PipelineExecution) -> bool:
        """Evaluate all quality gates for the execution"""
        all_passed = True
        
        for gate_name, quality_gate in self.quality_gates.items():
            try:
                passed = quality_gate.evaluate(execution)
                execution.metrics[f'quality_gate_{gate_name}'] = passed
                
                if not passed:
                    all_passed = False
                    execution.logs.append(f"Quality gate failed: {gate_name}")
                else:
                    execution.logs.append(f"Quality gate passed: {gate_name}")
                    
            except Exception as e:
                all_passed = False
                execution.logs.append(f"Quality gate error: {gate_name} - {str(e)}")
                self.logger.error(f"Error evaluating quality gate {gate_name}: {e}")
        
        return all_passed
    
    def get_execution_status(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """Get execution status"""
        execution = self.executions.get(execution_id)
        return execution.to_dict() if execution else None
    
    def get_pipeline_history(self, pipeline_name: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Get pipeline execution history"""
        pipeline_executions = [
            exec for exec in self.executions.values()
            if exec.pipeline_name == pipeline_name
        ]
        
        # Sort by start time, most recent first
        pipeline_executions.sort(key=lambda x: x.started_at, reverse=True)
        
        return [exec.to_dict() for exec in pipeline_executions[:limit]]
    
    def get_environment_status(self, environment: EnvironmentType) -> Dict[str, Any]:
        """Get environment status summary"""
        env_executions = [
            exec for exec in self.executions.values()
            if exec.environment == environment
        ]
        
        # Calculate statistics
        total_executions = len(env_executions)
        successful_executions = len([e for e in env_executions if e.status == PipelineStatus.SUCCESS])
        failed_executions = len([e for e in env_executions if e.status == PipelineStatus.FAILED])
        running_executions = len([e for e in env_executions if e.status == PipelineStatus.RUNNING])
        
        success_rate = successful_executions / total_executions if total_executions > 0 else 0
        
        # Calculate average duration
        completed_executions = [e for e in env_executions if e.duration_seconds is not None]
        avg_duration = sum(e.duration_seconds for e in completed_executions) / len(completed_executions) if completed_executions else 0
        
        return {
            'environment': environment.value,
            'total_executions': total_executions,
            'successful_executions': successful_executions,
            'failed_executions': failed_executions,
            'running_executions': running_executions,
            'success_rate': success_rate,
            'average_duration_seconds': avg_duration,
            'last_execution': max(env_executions, key=lambda x: x.started_at).to_dict() if env_executions else None
        }

# Example DataOps configuration
def create_example_dataops_config():
    """Create example DataOps configuration"""
    
    config_data = {
        'project': {
            'name': 'data-pipeline-project',
            'version': '1.0.0'
        },
        'environments': {
            'development': {
                'database_url': 'postgresql://dev:password@localhost:5432/dev_db',
                'storage_bucket': 'dev-data-bucket',
                'compute_resources': {'cpu': 2, 'memory': '4GB'}
            },
            'staging': {
                'database_url': 'postgresql://staging:password@staging-db:5432/staging_db',
                'storage_bucket': 'staging-data-bucket',
                'compute_resources': {'cpu': 4, 'memory': '8GB'}
            },
            'production': {
                'database_url': 'postgresql://prod:password@prod-db:5432/prod_db',
                'storage_bucket': 'prod-data-bucket',
                'compute_resources': {'cpu': 8, 'memory': '16GB'}
            }
        },
        'pipelines': {
            'customer_data_pipeline': {
                'description': 'Process customer data from various sources',
                'steps': [
                    {
                        'name': 'extract_data',
                        'type': 'python',
                        'script': 'scripts/extract_customer_data.py'
                    },
                    {
                        'name': 'transform_data',
                        'type': 'python',
                        'script': 'scripts/transform_customer_data.py'
                    },
                    {
                        'name': 'load_data',
                        'type': 'python',
                        'script': 'scripts/load_customer_data.py'
                    }
                ]
            },
            'sales_analytics_pipeline': {
                'description': 'Generate sales analytics and reports',
                'steps': [
                    {
                        'name': 'extract_sales_data',
                        'type': 'shell',
                        'command': 'python scripts/extract_sales.py --date=$(date +%Y-%m-%d)'
                    },
                    {
                        'name': 'generate_reports',
                        'type': 'docker',
                        'image': 'analytics:latest',
                        'command': 'python generate_reports.py',
                        'volumes': {'/data': {'bind': '/app/data', 'mode': 'rw'}}
                    }
                ]
            }
        },
        'quality_gates': {
            'data_quality_check': {
                'type': 'data_quality',
                'completeness_threshold': 0.95,
                'accuracy_threshold': 0.98,
                'consistency_threshold': 0.95
            },
            'performance_check': {
                'type': 'performance',
                'max_duration_minutes': 30,
                'max_memory_mb': 1024,
                'min_throughput_records_per_second': 500
            }
        },
        'monitoring': {
            'metrics_retention_days': 30,
            'alert_thresholds': {
                'failure_rate': 0.1,
                'avg_duration_increase': 1.5
            }
        },
        'notifications': {
            'slack_webhook': 'https://hooks.slack.com/services/YOUR/WEBHOOK',
            'email_recipients': ['dataops-team@company.com']
        }
    }
    
    return DataOpsConfig(
        project_name=config_data['project']['name'],
        version=config_data['project']['version'],
        environments=config_data['environments'],
        pipelines=config_data['pipelines'],
        quality_gates=config_data['quality_gates'],
        monitoring=config_data['monitoring'],
        notifications=config_data['notifications']
    )

# Example usage
def example_dataops_orchestration():
    """Example of DataOps orchestration"""
    
    # Create configuration
    config = create_example_dataops_config()
    
    # Initialize orchestrator
    orchestrator = DataOpsOrchestrator(config)
    
    print("=== DATAOPS ORCHESTRATION SIMULATION ===")
    
    # Execute pipelines in different environments
    executions = []
    
    # Development environment
    dev_execution = orchestrator.execute_pipeline(
        pipeline_name='customer_data_pipeline',
        environment=EnvironmentType.DEVELOPMENT,
        triggered_by='developer@company.com',
        commit_hash='abc123'
    )
    executions.append(dev_execution)
    
    # Staging environment
    staging_execution = orchestrator.execute_pipeline(
        pipeline_name='sales_analytics_pipeline',
        environment=EnvironmentType.STAGING,
        triggered_by='ci_cd_system',
        commit_hash='def456'
    )
    executions.append(staging_execution)
    
    print(f"\nExecuted {len(executions)} pipelines")
    
    # Check execution status
    for execution_id in executions:
        status = orchestrator.get_execution_status(execution_id)
        if status:
            print(f"\nExecution: {execution_id}")
            print(f"  Pipeline: {status['pipeline_name']}")
            print(f"  Environment: {status['environment']}")
            print(f"  Status: {status['status']}")
            print(f"  Duration: {status['duration_seconds']:.1f}s" if status['duration_seconds'] else "  Duration: N/A")
    
    # Get environment status
    for env in [EnvironmentType.DEVELOPMENT, EnvironmentType.STAGING]:
        env_status = orchestrator.get_environment_status(env)
        print(f"\n{env.value.title()} Environment:")
        print(f"  Total executions: {env_status['total_executions']}")
        print(f"  Success rate: {env_status['success_rate']:.1%}")
        print(f"  Average duration: {env_status['average_duration_seconds']:.1f}s")
    
    return orchestrator

# orchestrator = example_dataops_orchestration()
```

## DataOps Best Practices

### Core Principles

1. **Collaboration and Communication**
   - Foster cross-functional collaboration between data engineers, analysts, and business users
   - Implement clear communication channels and documentation standards
   - Establish shared responsibility for data quality and pipeline reliability

2. **Automation and Orchestration**
   - Automate repetitive tasks and manual processes
   - Implement automated testing and validation
   - Use orchestration tools for complex workflow management

3. **Version Control and Reproducibility**
   - Version control all data pipeline code and configurations
   - Ensure reproducible builds and deployments
   - Maintain environment consistency across development, staging, and production

4. **Continuous Integration and Deployment**
   - Implement CI/CD pipelines for data workflows
   - Automate testing and quality gates
   - Enable rapid, reliable deployments

5. **Monitoring and Observability**
   - Implement comprehensive monitoring and alerting
   - Track data quality, performance, and business metrics
   - Maintain detailed logs and audit trails

## Summary

DataOps Principles provide:

- **Collaborative Framework** - Cross-functional team coordination and communication
- **Automated Orchestration** - Pipeline execution with quality gates and monitoring
- **Configuration Management** - Environment-specific configurations and deployments
- **Quality Assurance** - Automated testing and validation frameworks

Key components:
- Pipeline orchestration engine with execution tracking
- Quality gates for data quality and performance validation
- Environment management and configuration
- Comprehensive logging and monitoring

---

**Next**: Learn about [CI/CD for Data Pipelines](/chapters/devops/cicd-pipelines) for automated pipeline deployment.
