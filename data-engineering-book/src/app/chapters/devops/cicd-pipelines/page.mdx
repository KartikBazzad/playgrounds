import {MermaidDiagram} from '@/components/MermaidDiagram';

# CI/CD for Data Pipelines

Continuous Integration and Continuous Deployment (CI/CD) for data pipelines ensures automated testing, validation, and deployment of data workflows. This approach enables rapid, reliable delivery of data pipeline changes while maintaining quality and consistency across environments.

## CI/CD Pipeline Architecture

<MermaidDiagram chart={`
graph TB
    subgraph "Source Control"
        SC1[Git Repository]
        SC2[Feature Branches]
        SC3[Pull Requests]
        SC4[Code Reviews]
    end
    
    subgraph "Continuous Integration"
        CI1[Automated Build]
        CI2[Unit Tests]
        CI3[Integration Tests]
        CI4[Data Quality Tests]
        CI5[Security Scans]
    end
    
    subgraph "Artifact Management"
        AM1[Build Artifacts]
        AM2[Container Images]
        AM3[Configuration Files]
        AM4[Test Reports]
    end
    
    subgraph "Continuous Deployment"
        CD1[Development Deploy]
        CD2[Staging Deploy]
        CD3[Production Deploy]
        CD4[Rollback Capability]
    end
    
    subgraph "Monitoring & Feedback"
        MF1[Pipeline Monitoring]
        MF2[Performance Metrics]
        MF3[Alert Management]
        MF4[Feedback Loop]
    end
    
    SC1 --> CI1
    SC2 --> CI2
    SC3 --> CI3
    SC4 --> CI4
    CI5 --> AM1
    
    CI1 --> AM1
    CI2 --> AM2
    CI3 --> AM3
    CI4 --> AM4
    
    AM1 --> CD1
    AM2 --> CD2
    AM3 --> CD3
    AM4 --> CD4
    
    CD1 --> MF1
    CD2 --> MF2
    CD3 --> MF3
    CD4 --> MF4
    
    MF4 --> SC1
    
    style SC1 fill:#e3f2fd
    style CI1 fill:#e8f5e8
    style AM1 fill:#fff3e0
    style CD1 fill:#f3e5f5
    style MF1 fill:#fce4ec
`} />

## CI/CD Framework Implementation

### Pipeline Automation System

```python
# Comprehensive CI/CD Framework for Data Pipelines
import os
import yaml
import json
import subprocess
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import git
import docker
import requests
from abc import ABC, abstractmethod

class BuildStatus(Enum):
    """Build status enumeration"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"

class DeploymentStage(Enum):
    """Deployment stage enumeration"""
    DEVELOPMENT = "development"
    TESTING = "testing"
    STAGING = "staging"
    PRODUCTION = "production"

class TestType(Enum):
    """Test type enumeration"""
    UNIT = "unit"
    INTEGRATION = "integration"
    DATA_QUALITY = "data_quality"
    PERFORMANCE = "performance"
    SECURITY = "security"

@dataclass
class BuildConfiguration:
    """Build configuration"""
    project_name: str
    version: str
    build_commands: List[str]
    test_commands: Dict[str, List[str]]
    quality_gates: Dict[str, Any]
    deployment_targets: Dict[str, Dict[str, Any]]
    notifications: Dict[str, Any]
    
    @classmethod
    def from_yaml(cls, config_path: str) -> 'BuildConfiguration':
        """Load build configuration from YAML"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        return cls(
            project_name=config['project']['name'],
            version=config['project']['version'],
            build_commands=config.get('build', {}).get('commands', []),
            test_commands=config.get('tests', {}),
            quality_gates=config.get('quality_gates', {}),
            deployment_targets=config.get('deployment', {}),
            notifications=config.get('notifications', {})
        )

@dataclass
class BuildExecution:
    """Build execution record"""
    build_id: str
    commit_hash: str
    branch: str
    triggered_by: str
    status: BuildStatus
    started_at: datetime
    completed_at: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    build_logs: List[str] = field(default_factory=list)
    test_results: Dict[str, Any] = field(default_factory=dict)
    artifacts: List[str] = field(default_factory=list)
    quality_gate_results: Dict[str, bool] = field(default_factory=dict)
    deployment_results: Dict[str, Any] = field(default_factory=dict)
    
    def complete(self, status: BuildStatus):
        """Mark build as complete"""
        self.completed_at = datetime.now()
        self.status = status
        if self.started_at:
            self.duration_seconds = (self.completed_at - self.started_at).total_seconds()
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'build_id': self.build_id,
            'commit_hash': self.commit_hash,
            'branch': self.branch,
            'triggered_by': self.triggered_by,
            'status': self.status.value,
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'duration_seconds': self.duration_seconds,
            'test_results': self.test_results,
            'artifacts_count': len(self.artifacts),
            'quality_gates_passed': all(self.quality_gate_results.values()),
            'deployments': list(self.deployment_results.keys())
        }

class TestRunner(ABC):
    """Abstract test runner"""
    
    def __init__(self, test_type: TestType, config: Dict[str, Any]):
        self.test_type = test_type
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    @abstractmethod
    def run_tests(self, build_execution: BuildExecution) -> Dict[str, Any]:
        """Run tests and return results"""
        pass

class UnitTestRunner(TestRunner):
    """Unit test runner"""
    
    def run_tests(self, build_execution: BuildExecution) -> Dict[str, Any]:
        """Run unit tests"""
        try:
            commands = self.config.get('commands', ['python -m pytest tests/unit/'])
            
            results = {
                'test_type': self.test_type.value,
                'passed': 0,
                'failed': 0,
                'skipped': 0,
                'coverage': 0.0,
                'duration': 0.0,
                'details': []
            }
            
            for command in commands:
                self.logger.info(f"Running unit test command: {command}")
                
                start_time = datetime.now()
                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=1800  # 30 minutes
                )
                
                duration = (datetime.now() - start_time).total_seconds()
                results['duration'] += duration
                
                if result.returncode == 0:
                    # Parse pytest output for test counts
                    output = result.stdout
                    if 'passed' in output:
                        # Simple parsing - in real implementation, use pytest-json-report
                        results['passed'] += 1
                    
                    build_execution.build_logs.append(f"Unit tests passed: {command}")
                else:
                    results['failed'] += 1
                    build_execution.build_logs.append(f"Unit tests failed: {command}")
                    build_execution.build_logs.append(f"Error: {result.stderr}")
                
                results['details'].append({
                    'command': command,
                    'returncode': result.returncode,
                    'duration': duration,
                    'stdout': result.stdout[:1000],  # Truncate for storage
                    'stderr': result.stderr[:1000]
                })
            
            # Calculate overall success
            results['success'] = results['failed'] == 0
            
            return results
            
        except Exception as e:
            self.logger.error(f"Unit test execution failed: {e}")
            return {
                'test_type': self.test_type.value,
                'success': False,
                'error': str(e),
                'passed': 0,
                'failed': 1,
                'duration': 0.0
            }

class DataQualityTestRunner(TestRunner):
    """Data quality test runner"""
    
    def run_tests(self, build_execution: BuildExecution) -> Dict[str, Any]:
        """Run data quality tests"""
        try:
            results = {
                'test_type': self.test_type.value,
                'checks_passed': 0,
                'checks_failed': 0,
                'quality_score': 0.0,
                'duration': 0.0,
                'details': []
            }
            
            # Run data quality checks
            quality_checks = self.config.get('checks', [])
            
            start_time = datetime.now()
            
            for check in quality_checks:
                check_name = check.get('name', 'unnamed_check')
                check_query = check.get('query')
                check_threshold = check.get('threshold', 0.95)
                
                self.logger.info(f"Running data quality check: {check_name}")
                
                try:
                    # In real implementation, execute against actual database
                    # Here we simulate the check
                    import random
                    quality_score = random.uniform(0.8, 1.0)
                    
                    check_result = {
                        'name': check_name,
                        'score': quality_score,
                        'threshold': check_threshold,
                        'passed': quality_score >= check_threshold
                    }
                    
                    if check_result['passed']:
                        results['checks_passed'] += 1
                    else:
                        results['checks_failed'] += 1
                    
                    results['details'].append(check_result)
                    
                except Exception as e:
                    results['checks_failed'] += 1
                    results['details'].append({
                        'name': check_name,
                        'error': str(e),
                        'passed': False
                    })
            
            duration = (datetime.now() - start_time).total_seconds()
            results['duration'] = duration
            
            # Calculate overall quality score
            total_checks = results['checks_passed'] + results['checks_failed']
            if total_checks > 0:
                results['quality_score'] = results['checks_passed'] / total_checks
            
            results['success'] = results['checks_failed'] == 0
            
            build_execution.build_logs.append(
                f"Data quality tests: {results['checks_passed']} passed, "
                f"{results['checks_failed']} failed"
            )
            
            return results
            
        except Exception as e:
            self.logger.error(f"Data quality test execution failed: {e}")
            return {
                'test_type': self.test_type.value,
                'success': False,
                'error': str(e),
                'checks_passed': 0,
                'checks_failed': 1,
                'duration': 0.0
            }

class IntegrationTestRunner(TestRunner):
    """Integration test runner"""
    
    def run_tests(self, build_execution: BuildExecution) -> Dict[str, Any]:
        """Run integration tests"""
        try:
            results = {
                'test_type': self.test_type.value,
                'scenarios_passed': 0,
                'scenarios_failed': 0,
                'duration': 0.0,
                'details': []
            }
            
            test_scenarios = self.config.get('scenarios', [])
            
            start_time = datetime.now()
            
            for scenario in test_scenarios:
                scenario_name = scenario.get('name', 'unnamed_scenario')
                scenario_steps = scenario.get('steps', [])
                
                self.logger.info(f"Running integration test scenario: {scenario_name}")
                
                scenario_result = {
                    'name': scenario_name,
                    'steps_completed': 0,
                    'steps_total': len(scenario_steps),
                    'passed': True,
                    'error': None
                }
                
                try:
                    for step in scenario_steps:
                        step_command = step.get('command')
                        step_timeout = step.get('timeout', 300)
                        
                        result = subprocess.run(
                            step_command,
                            shell=True,
                            capture_output=True,
                            text=True,
                            timeout=step_timeout
                        )
                        
                        if result.returncode != 0:
                            scenario_result['passed'] = False
                            scenario_result['error'] = result.stderr
                            break
                        
                        scenario_result['steps_completed'] += 1
                    
                    if scenario_result['passed']:
                        results['scenarios_passed'] += 1
                    else:
                        results['scenarios_failed'] += 1
                    
                except Exception as e:
                    scenario_result['passed'] = False
                    scenario_result['error'] = str(e)
                    results['scenarios_failed'] += 1
                
                results['details'].append(scenario_result)
            
            duration = (datetime.now() - start_time).total_seconds()
            results['duration'] = duration
            results['success'] = results['scenarios_failed'] == 0
            
            build_execution.build_logs.append(
                f"Integration tests: {results['scenarios_passed']} passed, "
                f"{results['scenarios_failed']} failed"
            )
            
            return results
            
        except Exception as e:
            self.logger.error(f"Integration test execution failed: {e}")
            return {
                'test_type': self.test_type.value,
                'success': False,
                'error': str(e),
                'scenarios_passed': 0,
                'scenarios_failed': 1,
                'duration': 0.0
            }

class CICDPipeline:
    """Main CI/CD pipeline orchestrator"""
    
    def __init__(self, config: BuildConfiguration):
        self.config = config
        self.builds: Dict[str, BuildExecution] = {}
        self.test_runners: Dict[TestType, TestRunner] = {}
        self.logger = logging.getLogger(__name__)
        
        # Initialize test runners
        self._initialize_test_runners()
    
    def _initialize_test_runners(self):
        """Initialize test runners based on configuration"""
        test_configs = self.config.test_commands
        
        for test_type_str, test_config in test_configs.items():
            try:
                test_type = TestType(test_type_str)
                
                if test_type == TestType.UNIT:
                    self.test_runners[test_type] = UnitTestRunner(test_type, test_config)
                elif test_type == TestType.DATA_QUALITY:
                    self.test_runners[test_type] = DataQualityTestRunner(test_type, test_config)
                elif test_type == TestType.INTEGRATION:
                    self.test_runners[test_type] = IntegrationTestRunner(test_type, test_config)
                # Add more test runners as needed
                
            except ValueError:
                self.logger.warning(f"Unknown test type: {test_type_str}")
    
    def trigger_build(self, commit_hash: str, branch: str, triggered_by: str) -> str:
        """Trigger a new build"""
        build_id = f"build_{commit_hash[:8]}_{int(datetime.now().timestamp())}"
        
        build_execution = BuildExecution(
            build_id=build_id,
            commit_hash=commit_hash,
            branch=branch,
            triggered_by=triggered_by,
            status=BuildStatus.PENDING,
            started_at=datetime.now()
        )
        
        self.builds[build_id] = build_execution
        
        try:
            # Start build process
            build_execution.status = BuildStatus.RUNNING
            self.logger.info(f"Starting build: {build_id}")
            
            # Execute build steps
            self._execute_build_steps(build_execution)
            
            # Run tests
            self._run_tests(build_execution)
            
            # Evaluate quality gates
            quality_passed = self._evaluate_quality_gates(build_execution)
            
            if quality_passed:
                # Deploy to environments
                self._deploy_to_environments(build_execution)
                
                build_execution.complete(BuildStatus.SUCCESS)
                self.logger.info(f"Build completed successfully: {build_id}")
            else:
                build_execution.complete(BuildStatus.FAILED)
                self.logger.error(f"Build failed quality gates: {build_id}")
            
        except Exception as e:
            build_execution.complete(BuildStatus.FAILED)
            build_execution.build_logs.append(f"Build failed: {str(e)}")
            self.logger.error(f"Build execution failed: {build_id} - {e}")
        
        return build_id
    
    def _execute_build_steps(self, build_execution: BuildExecution):
        """Execute build steps"""
        for command in self.config.build_commands:
            build_execution.build_logs.append(f"Executing: {command}")
            self.logger.info(f"Executing build command: {command}")
            
            try:
                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=1800
                )
                
                if result.returncode != 0:
                    raise subprocess.CalledProcessError(result.returncode, command, result.stderr)
                
                build_execution.build_logs.append(f"Command completed: {command}")
                
            except subprocess.TimeoutExpired:
                raise Exception(f"Build command timed out: {command}")
    
    def _run_tests(self, build_execution: BuildExecution):
        """Run all configured tests"""
        for test_type, test_runner in self.test_runners.items():
            self.logger.info(f"Running {test_type.value} tests")
            
            try:
                test_results = test_runner.run_tests(build_execution)
                build_execution.test_results[test_type.value] = test_results
                
                if not test_results.get('success', False):
                    build_execution.build_logs.append(f"{test_type.value} tests failed")
                else:
                    build_execution.build_logs.append(f"{test_type.value} tests passed")
                
            except Exception as e:
                build_execution.test_results[test_type.value] = {
                    'success': False,
                    'error': str(e)
                }
                build_execution.build_logs.append(f"{test_type.value} tests error: {str(e)}")
    
    def _evaluate_quality_gates(self, build_execution: BuildExecution) -> bool:
        """Evaluate quality gates"""
        all_passed = True
        
        for gate_name, gate_config in self.config.quality_gates.items():
            try:
                gate_type = gate_config.get('type')
                
                if gate_type == 'test_coverage':
                    threshold = gate_config.get('threshold', 0.8)
                    # In real implementation, parse coverage from test results
                    coverage = 0.85  # Simulated
                    passed = coverage >= threshold
                    
                elif gate_type == 'test_success_rate':
                    threshold = gate_config.get('threshold', 1.0)
                    # Calculate test success rate
                    total_tests = sum(
                        result.get('passed', 0) + result.get('failed', 0) + result.get('scenarios_passed', 0) + result.get('scenarios_failed', 0)
                        for result in build_execution.test_results.values()
                    )
                    passed_tests = sum(
                        result.get('passed', 0) + result.get('scenarios_passed', 0)
                        for result in build_execution.test_results.values()
                    )
                    
                    success_rate = passed_tests / total_tests if total_tests > 0 else 0
                    passed = success_rate >= threshold
                    
                elif gate_type == 'data_quality_score':
                    threshold = gate_config.get('threshold', 0.95)
                    dq_results = build_execution.test_results.get('data_quality', {})
                    quality_score = dq_results.get('quality_score', 0)
                    passed = quality_score >= threshold
                    
                else:
                    passed = True  # Unknown gate type passes by default
                
                build_execution.quality_gate_results[gate_name] = passed
                
                if not passed:
                    all_passed = False
                    build_execution.build_logs.append(f"Quality gate failed: {gate_name}")
                else:
                    build_execution.build_logs.append(f"Quality gate passed: {gate_name}")
                
            except Exception as e:
                all_passed = False
                build_execution.quality_gate_results[gate_name] = False
                build_execution.build_logs.append(f"Quality gate error: {gate_name} - {str(e)}")
                self.logger.error(f"Error evaluating quality gate {gate_name}: {e}")
        
        return all_passed
    
    def _deploy_to_environments(self, build_execution: BuildExecution):
        """Deploy to configured environments"""
        for env_name, env_config in self.config.deployment_targets.items():
            try:
                self.logger.info(f"Deploying to {env_name}")
                
                # Simulate deployment
                deployment_result = {
                    'environment': env_name,
                    'status': 'success',
                    'deployed_at': datetime.now().isoformat(),
                    'deployment_url': env_config.get('url', f'https://{env_name}.example.com')
                }
                
                build_execution.deployment_results[env_name] = deployment_result
                build_execution.build_logs.append(f"Deployed to {env_name}")
                
            except Exception as e:
                deployment_result = {
                    'environment': env_name,
                    'status': 'failed',
                    'error': str(e)
                }
                build_execution.deployment_results[env_name] = deployment_result
                build_execution.build_logs.append(f"Deployment to {env_name} failed: {str(e)}")
                self.logger.error(f"Deployment to {env_name} failed: {e}")
    
    def get_build_status(self, build_id: str) -> Optional[Dict[str, Any]]:
        """Get build status"""
        build = self.builds.get(build_id)
        return build.to_dict() if build else None
    
    def get_build_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get build history"""
        builds = list(self.builds.values())
        builds.sort(key=lambda x: x.started_at, reverse=True)
        return [build.to_dict() for build in builds[:limit]]
    
    def get_pipeline_metrics(self) -> Dict[str, Any]:
        """Get pipeline metrics"""
        builds = list(self.builds.values())
        
        if not builds:
            return {'message': 'No builds found'}
        
        total_builds = len(builds)
        successful_builds = len([b for b in builds if b.status == BuildStatus.SUCCESS])
        failed_builds = len([b for b in builds if b.status == BuildStatus.FAILED])
        
        success_rate = successful_builds / total_builds if total_builds > 0 else 0
        
        # Calculate average build time
        completed_builds = [b for b in builds if b.duration_seconds is not None]
        avg_build_time = sum(b.duration_seconds for b in completed_builds) / len(completed_builds) if completed_builds else 0
        
        return {
            'total_builds': total_builds,
            'successful_builds': successful_builds,
            'failed_builds': failed_builds,
            'success_rate': success_rate,
            'average_build_time_seconds': avg_build_time,
            'last_build': builds[0].to_dict() if builds else None
        }

# Example CI/CD configuration
def create_example_cicd_config():
    """Create example CI/CD configuration"""
    
    return BuildConfiguration(
        project_name='data-pipeline-project',
        version='1.0.0',
        build_commands=[
            'pip install -r requirements.txt',
            'python -m pip install -e .',
            'python setup.py build'
        ],
        test_commands={
            'unit': {
                'commands': ['python -m pytest tests/unit/ --cov=src --cov-report=xml']
            },
            'integration': {
                'scenarios': [
                    {
                        'name': 'end_to_end_pipeline',
                        'steps': [
                            {'command': 'python scripts/setup_test_data.py'},
                            {'command': 'python scripts/run_pipeline.py --env=test'},
                            {'command': 'python scripts/validate_output.py'}
                        ]
                    }
                ]
            },
            'data_quality': {
                'checks': [
                    {
                        'name': 'completeness_check',
                        'query': 'SELECT COUNT(*) FROM test_table WHERE column IS NOT NULL',
                        'threshold': 0.95
                    },
                    {
                        'name': 'uniqueness_check',
                        'query': 'SELECT COUNT(DISTINCT id) / COUNT(*) FROM test_table',
                        'threshold': 1.0
                    }
                ]
            }
        },
        quality_gates={
            'test_coverage': {
                'type': 'test_coverage',
                'threshold': 0.8
            },
            'test_success_rate': {
                'type': 'test_success_rate',
                'threshold': 1.0
            },
            'data_quality_score': {
                'type': 'data_quality_score',
                'threshold': 0.95
            }
        },
        deployment_targets={
            'development': {
                'url': 'https://dev.example.com',
                'auto_deploy': True
            },
            'staging': {
                'url': 'https://staging.example.com',
                'auto_deploy': True,
                'requires_approval': False
            },
            'production': {
                'url': 'https://prod.example.com',
                'auto_deploy': False,
                'requires_approval': True
            }
        },
        notifications={
            'slack_webhook': 'https://hooks.slack.com/services/YOUR/WEBHOOK',
            'email_recipients': ['team@company.com']
        }
    )

# Example usage
def example_cicd_pipeline():
    """Example of CI/CD pipeline execution"""
    
    # Create configuration
    config = create_example_cicd_config()
    
    # Initialize CI/CD pipeline
    pipeline = CICDPipeline(config)
    
    print("=== CI/CD PIPELINE SIMULATION ===")
    
    # Trigger builds
    builds = []
    
    # Feature branch build
    feature_build = pipeline.trigger_build(
        commit_hash='abc123def456',
        branch='feature/new-data-source',
        triggered_by='developer@company.com'
    )
    builds.append(feature_build)
    
    # Main branch build
    main_build = pipeline.trigger_build(
        commit_hash='def456ghi789',
        branch='main',
        triggered_by='ci_system'
    )
    builds.append(main_build)
    
    print(f"\nTriggered {len(builds)} builds")
    
    # Check build status
    for build_id in builds:
        status = pipeline.get_build_status(build_id)
        if status:
            print(f"\nBuild: {build_id}")
            print(f"  Status: {status['status']}")
            print(f"  Branch: {status['branch']}")
            print(f"  Duration: {status['duration_seconds']:.1f}s" if status['duration_seconds'] else "  Duration: N/A")
            print(f"  Quality gates passed: {status['quality_gates_passed']}")
            print(f"  Deployments: {', '.join(status['deployments'])}")
    
    # Get pipeline metrics
    metrics = pipeline.get_pipeline_metrics()
    print(f"\n=== PIPELINE METRICS ===")
    print(f"Total builds: {metrics['total_builds']}")
    print(f"Success rate: {metrics['success_rate']:.1%}")
    print(f"Average build time: {metrics['average_build_time_seconds']:.1f}s")
    
    return pipeline

# pipeline = example_cicd_pipeline()
```

## Best Practices

### CI/CD Implementation Guidelines

1. **Automated Testing Strategy**
   - Implement comprehensive test suites (unit, integration, data quality)
   - Use test-driven development for data pipeline components
   - Maintain high test coverage and quality standards
   - Implement automated performance and security testing

2. **Quality Gates and Governance**
   - Define clear quality criteria for each stage
   - Implement automated quality gates
   - Require code reviews and approvals
   - Maintain audit trails and compliance records

3. **Environment Management**
   - Use infrastructure as code for consistent environments
   - Implement environment-specific configurations
   - Automate environment provisioning and teardown
   - Maintain environment parity across stages

4. **Deployment Strategies**
   - Implement blue-green or canary deployments
   - Use feature flags for controlled rollouts
   - Maintain rollback capabilities
   - Monitor deployments with automated health checks

## Summary

CI/CD for Data Pipelines provides:

- **Automated Build Pipeline** - Comprehensive build and test automation
- **Quality Assurance** - Multi-layered testing and quality gates
- **Deployment Automation** - Automated deployment across environments
- **Monitoring and Feedback** - Continuous monitoring and improvement

Key components:
- Build orchestration with comprehensive logging
- Multiple test runner implementations
- Quality gate evaluation system
- Multi-environment deployment automation

---

**Next**: Learn about [Infrastructure as Code](/chapters/devops/infrastructure-as-code) for automated infrastructure management.
