import {MermaidDiagram} from '@/components/MermaidDiagram';

# Scaling Data Systems

This case study explores strategies for scaling data systems to handle exponential growth in data volume, velocity, and complexity. It covers horizontal and vertical scaling, distributed architectures, performance optimization, and capacity planning for enterprise-scale data platforms.

## Scaling Architecture Overview

<MermaidDiagram chart={`
graph TB
    subgraph "Load Balancing Layer"
        LB1[Application Load Balancer]
        LB2[Database Load Balancer]
        LB3[Cache Load Balancer]
    end
    
    subgraph "Application Tier"
        APP1[App Server 1]
        APP2[App Server 2]
        APP3[App Server N]
    end
    
    subgraph "Caching Layer"
        CACHE1[Redis Cluster 1]
        CACHE2[Redis Cluster 2]
        CACHE3[Memcached Pool]
    end
    
    subgraph "Database Tier"
        DB1[Primary Database]
        DB2[Read Replica 1]
        DB3[Read Replica 2]
        DB4[Sharded Database 1]
        DB5[Sharded Database 2]
    end
    
    subgraph "Storage Layer"
        S1[Distributed File System]
        S2[Object Storage]
        S3[Data Lake]
    end
    
    subgraph "Processing Layer"
        P1[Spark Cluster]
        P2[Kafka Cluster]
        P3[Elasticsearch Cluster]
    end
    
    LB1 --> APP1
    LB1 --> APP2
    LB1 --> APP3
    
    LB2 --> DB1
    LB2 --> DB2
    LB2 --> DB3
    
    LB3 --> CACHE1
    LB3 --> CACHE2
    LB3 --> CACHE3
    
    APP1 --> CACHE1
    APP2 --> CACHE2
    APP3 --> CACHE3
    
    DB1 --> DB4
    DB1 --> DB5
    
    DB1 --> S1
    DB2 --> S2
    DB3 --> S3
    
    P1 --> S1
    P2 --> S2
    P3 --> S3
    
    style LB1 fill:#e3f2fd
    style APP1 fill:#e8f5e8
    style CACHE1 fill:#fff3e0
    style DB1 fill:#f3e5f5
    style S1 fill:#fce4ec
    style P1 fill:#e0f2f1
`} />

## Scaling Framework Implementation

### Core Scaling Components

```python
# Data Systems Scaling Framework
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
import logging
import threading
import time
from concurrent.futures import ThreadPoolExecutor

class ScalingType(Enum):
    """Scaling type enumeration"""
    HORIZONTAL = "horizontal"
    VERTICAL = "vertical"
    HYBRID = "hybrid"

class ResourceType(Enum):
    """Resource type enumeration"""
    CPU = "cpu"
    MEMORY = "memory"
    STORAGE = "storage"
    DATABASE = "database"
    CACHE = "cache"

class ScalingTrigger(Enum):
    """Scaling trigger enumeration"""
    CPU_THRESHOLD = "cpu_threshold"
    MEMORY_THRESHOLD = "memory_threshold"
    QUEUE_LENGTH = "queue_length"
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"

@dataclass
class ResourceMetrics:
    """Resource utilization metrics"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    disk_io_percent: float
    network_io_mbps: float
    active_connections: int
    queue_length: int
    response_time_ms: float
    throughput_rps: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'cpu_percent': self.cpu_percent,
            'memory_percent': self.memory_percent,
            'disk_io_percent': self.disk_io_percent,
            'network_io_mbps': self.network_io_mbps,
            'active_connections': self.active_connections,
            'queue_length': self.queue_length,
            'response_time_ms': self.response_time_ms,
            'throughput_rps': self.throughput_rps
        }

@dataclass
class ScalingRule:
    """Scaling rule configuration"""
    rule_id: str
    name: str
    trigger: ScalingTrigger
    resource_type: ResourceType
    threshold_up: float
    threshold_down: float
    scaling_type: ScalingType
    min_instances: int
    max_instances: int
    cooldown_minutes: int
    enabled: bool = True
    
    def should_scale_up(self, current_value: float) -> bool:
        """Check if should scale up"""
        return self.enabled and current_value > self.threshold_up
    
    def should_scale_down(self, current_value: float) -> bool:
        """Check if should scale down"""
        return self.enabled and current_value < self.threshold_down

@dataclass
class ScalingAction:
    """Scaling action record"""
    action_id: str
    timestamp: datetime
    rule_id: str
    action_type: str  # scale_up, scale_down
    resource_type: ResourceType
    current_instances: int
    target_instances: int
    trigger_value: float
    status: str = "pending"
    error_message: Optional[str] = None
    completed_at: Optional[datetime] = None
    
    def complete_action(self, success: bool, error_message: str = None):
        """Complete scaling action"""
        self.status = "completed" if success else "failed"
        self.error_message = error_message
        self.completed_at = datetime.now()

class MetricsCollector:
    """System metrics collection"""
    
    def __init__(self, collection_interval_seconds: int = 30):
        self.collection_interval = collection_interval_seconds
        self.metrics_history: List[ResourceMetrics] = []
        self.is_collecting = False
        self.logger = logging.getLogger(__name__)
    
    def start_collection(self):
        """Start metrics collection"""
        self.is_collecting = True
        self.logger.info("Started metrics collection")
    
    def stop_collection(self):
        """Stop metrics collection"""
        self.is_collecting = False
        self.logger.info("Stopped metrics collection")
    
    def collect_current_metrics(self) -> ResourceMetrics:
        """Collect current system metrics"""
        # Simulate system metrics
        return ResourceMetrics(
            timestamp=datetime.now(),
            cpu_percent=np.random.uniform(20, 90),
            memory_percent=np.random.uniform(30, 85),
            disk_io_percent=np.random.uniform(10, 70),
            network_io_mbps=np.random.uniform(50, 500),
            active_connections=np.random.randint(50, 500),
            queue_length=np.random.randint(0, 100),
            response_time_ms=np.random.uniform(50, 500),
            throughput_rps=np.random.uniform(100, 1000)
        )
    
    def get_latest_metrics(self) -> Optional[ResourceMetrics]:
        """Get latest metrics"""
        if not self.metrics_history:
            # Generate sample metrics if none exist
            metrics = self.collect_current_metrics()
            self.metrics_history.append(metrics)
        return self.metrics_history[-1]
    
    def get_average_metrics(self, minutes: int = 5) -> Optional[ResourceMetrics]:
        """Get average metrics over time period"""
        if not self.metrics_history:
            return self.get_latest_metrics()
        
        # Return latest for simplicity in demo
        return self.metrics_history[-1]

class AutoScaler:
    """Automatic scaling engine"""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.scaling_rules: Dict[str, ScalingRule] = {}
        self.scaling_history: List[ScalingAction] = []
        self.current_instances: Dict[ResourceType, int] = {
            ResourceType.DATABASE: 3,
            ResourceType.CACHE: 2,
            ResourceType.CPU: 4,
            ResourceType.MEMORY: 4
        }
        self.last_scaling_time: Dict[str, datetime] = {}
        self.is_running = False
        self.logger = logging.getLogger(__name__)
    
    def add_scaling_rule(self, rule: ScalingRule):
        """Add scaling rule"""
        self.scaling_rules[rule.rule_id] = rule
        self.logger.info(f"Added scaling rule: {rule.name}")
    
    def start_autoscaling(self):
        """Start autoscaling"""
        self.is_running = True
        self.logger.info("Started autoscaling")
    
    def stop_autoscaling(self):
        """Stop autoscaling"""
        self.is_running = False
        self.logger.info("Stopped autoscaling")
    
    def evaluate_scaling_rules(self):
        """Evaluate all scaling rules"""
        current_metrics = self.metrics_collector.get_average_metrics()
        if not current_metrics:
            return
        
        for rule in self.scaling_rules.values():
            if not rule.enabled:
                continue
            
            # Get current value for the rule
            current_value = self._get_metric_value(current_metrics, rule)
            if current_value is None:
                continue
            
            # Check scaling conditions
            current_instances = self.current_instances.get(rule.resource_type, 1)
            
            if rule.should_scale_up(current_value) and current_instances < rule.max_instances:
                self._execute_scaling_action(rule, "scale_up", current_value, current_instances)
            elif rule.should_scale_down(current_value) and current_instances > rule.min_instances:
                self._execute_scaling_action(rule, "scale_down", current_value, current_instances)
    
    def _get_metric_value(self, metrics: ResourceMetrics, rule: ScalingRule) -> Optional[float]:
        """Get metric value for scaling rule"""
        if rule.trigger == ScalingTrigger.CPU_THRESHOLD:
            return metrics.cpu_percent
        elif rule.trigger == ScalingTrigger.MEMORY_THRESHOLD:
            return metrics.memory_percent
        elif rule.trigger == ScalingTrigger.QUEUE_LENGTH:
            return float(metrics.queue_length)
        elif rule.trigger == ScalingTrigger.RESPONSE_TIME:
            return metrics.response_time_ms
        elif rule.trigger == ScalingTrigger.THROUGHPUT:
            return metrics.throughput_rps
        return None
    
    def _execute_scaling_action(self, rule: ScalingRule, action_type: str, 
                               trigger_value: float, current_instances: int):
        """Execute scaling action"""
        try:
            # Calculate target instances
            if action_type == "scale_up":
                target_instances = min(current_instances + 1, rule.max_instances)
            else:  # scale_down
                target_instances = max(current_instances - 1, rule.min_instances)
            
            # Create scaling action
            action = ScalingAction(
                action_id=f"action_{len(self.scaling_history) + 1}",
                timestamp=datetime.now(),
                rule_id=rule.rule_id,
                action_type=action_type,
                resource_type=rule.resource_type,
                current_instances=current_instances,
                target_instances=target_instances,
                trigger_value=trigger_value
            )
            
            # Simulate scaling execution
            success = np.random.random() > 0.1  # 90% success rate
            action.complete_action(success)
            self.scaling_history.append(action)
            
            if success:
                self.current_instances[rule.resource_type] = target_instances
                self.last_scaling_time[rule.rule_id] = datetime.now()
                
                self.logger.info(
                    f"Scaling action completed: {action_type} {rule.resource_type.value} "
                    f"from {current_instances} to {target_instances} instances"
                )
            
        except Exception as e:
            self.logger.error(f"Error executing scaling action: {e}")
    
    def get_scaling_status(self) -> Dict[str, Any]:
        """Get current scaling status"""
        recent_actions = [
            action for action in self.scaling_history 
            if action.timestamp >= datetime.now() - timedelta(hours=1)
        ]
        
        return {
            'current_instances': {rt.value: count for rt, count in self.current_instances.items()},
            'active_rules': len([r for r in self.scaling_rules.values() if r.enabled]),
            'total_rules': len(self.scaling_rules),
            'recent_actions_count': len(recent_actions),
            'successful_actions': len([a for a in recent_actions if a.status == "completed"]),
            'failed_actions': len([a for a in recent_actions if a.status == "failed"])
        }

class CapacityPlanner:
    """Capacity planning and forecasting"""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.logger = logging.getLogger(__name__)
    
    def forecast_capacity_needs(self, days_ahead: int = 30) -> Dict[str, Any]:
        """Forecast capacity needs"""
        try:
            # Simulate forecasting with sample data
            current_metrics = self.metrics_collector.get_latest_metrics()
            if not current_metrics:
                return {'error': 'No metrics available'}
            
            # Simple growth projection
            growth_rate = 0.05  # 5% growth
            
            cpu_forecast = {
                'current': current_metrics.cpu_percent,
                'forecasted': min(current_metrics.cpu_percent * (1 + growth_rate), 100),
                'trend': 'increasing' if growth_rate > 0 else 'stable',
                'confidence': 0.8
            }
            
            memory_forecast = {
                'current': current_metrics.memory_percent,
                'forecasted': min(current_metrics.memory_percent * (1 + growth_rate), 100),
                'trend': 'increasing' if growth_rate > 0 else 'stable',
                'confidence': 0.8
            }
            
            throughput_forecast = {
                'current': current_metrics.throughput_rps,
                'forecasted': current_metrics.throughput_rps * (1 + growth_rate),
                'trend': 'increasing' if growth_rate > 0 else 'stable',
                'confidence': 0.8
            }
            
            # Generate recommendations
            recommendations = self._generate_capacity_recommendations(
                cpu_forecast, memory_forecast, throughput_forecast
            )
            
            return {
                'forecast_period_days': days_ahead,
                'forecasts': {
                    'cpu_percent': cpu_forecast,
                    'memory_percent': memory_forecast,
                    'throughput_rps': throughput_forecast
                },
                'recommendations': recommendations,
                'forecast_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Error forecasting capacity: {e}")
            return {'error': str(e)}
    
    def _generate_capacity_recommendations(self, cpu_forecast: Dict, 
                                         memory_forecast: Dict, 
                                         throughput_forecast: Dict) -> List[Dict[str, Any]]:
        """Generate capacity recommendations"""
        recommendations = []
        
        # CPU recommendations
        if cpu_forecast['forecasted'] > 80:
            recommendations.append({
                'type': 'scale_up',
                'resource': 'cpu',
                'priority': 'high',
                'reason': f"CPU usage forecasted to reach {cpu_forecast['forecasted']:.1f}%",
                'recommended_action': 'Add 2-3 CPU cores or scale horizontally'
            })
        
        # Memory recommendations
        if memory_forecast['forecasted'] > 85:
            recommendations.append({
                'type': 'scale_up',
                'resource': 'memory',
                'priority': 'high',
                'reason': f"Memory usage forecasted to reach {memory_forecast['forecasted']:.1f}%",
                'recommended_action': 'Increase memory allocation by 25-50%'
            })
        
        # Throughput recommendations
        if throughput_forecast['forecasted'] > 800:
            recommendations.append({
                'type': 'scale_up',
                'resource': 'infrastructure',
                'priority': 'medium',
                'reason': f"Throughput forecasted to reach {throughput_forecast['forecasted']:.0f} RPS",
                'recommended_action': 'Plan for horizontal scaling and load balancing'
            })
        
        # Default recommendation
        if not recommendations:
            recommendations.append({
                'type': 'maintain',
                'resource': 'all',
                'priority': 'low',
                'reason': 'Current capacity appears sufficient',
                'recommended_action': 'Continue monitoring current configuration'
            })
        
        return recommendations

class ScalingOrchestrator:
    """Main scaling orchestration system"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.autoscaler = AutoScaler(self.metrics_collector)
        self.capacity_planner = CapacityPlanner(self.metrics_collector)
        self.logger = logging.getLogger(__name__)
    
    def initialize_scaling_system(self):
        """Initialize complete scaling system"""
        # Start metrics collection
        self.metrics_collector.start_collection()
        
        # Add default scaling rules
        self._add_default_scaling_rules()
        
        # Start autoscaling
        self.autoscaler.start_autoscaling()
        
        self.logger.info("Scaling system initialized")
    
    def _add_default_scaling_rules(self):
        """Add default scaling rules"""
        
        # CPU scaling rule
        cpu_rule = ScalingRule(
            rule_id="cpu_scaling",
            name="CPU Threshold Scaling",
            trigger=ScalingTrigger.CPU_THRESHOLD,
            resource_type=ResourceType.CPU,
            threshold_up=75.0,
            threshold_down=25.0,
            scaling_type=ScalingType.HORIZONTAL,
            min_instances=2,
            max_instances=10,
            cooldown_minutes=5
        )
        self.autoscaler.add_scaling_rule(cpu_rule)
        
        # Memory scaling rule
        memory_rule = ScalingRule(
            rule_id="memory_scaling",
            name="Memory Threshold Scaling",
            trigger=ScalingTrigger.MEMORY_THRESHOLD,
            resource_type=ResourceType.MEMORY,
            threshold_up=80.0,
            threshold_down=30.0,
            scaling_type=ScalingType.VERTICAL,
            min_instances=2,
            max_instances=8,
            cooldown_minutes=10
        )
        self.autoscaler.add_scaling_rule(memory_rule)
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        latest_metrics = self.metrics_collector.get_latest_metrics()
        scaling_status = self.autoscaler.get_scaling_status()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'metrics_collection_active': self.metrics_collector.is_collecting,
            'autoscaling_active': self.autoscaler.is_running,
            'latest_metrics': latest_metrics.to_dict() if latest_metrics else None,
            'scaling_status': scaling_status
        }
    
    def shutdown_scaling_system(self):
        """Shutdown scaling system"""
        self.autoscaler.stop_autoscaling()
        self.metrics_collector.stop_collection()
        self.logger.info("Scaling system shutdown")

# Example usage
def example_scaling_system():
    """Example of comprehensive scaling system"""
    
    print("=== DATA SYSTEMS SCALING CASE STUDY ===")
    
    # Initialize scaling orchestrator
    orchestrator = ScalingOrchestrator()
    orchestrator.initialize_scaling_system()
    
    print("Scaling system initialized with default rules")
    
    # Simulate some scaling decisions
    orchestrator.autoscaler.evaluate_scaling_rules()
    
    # Get system status
    status = orchestrator.get_system_status()
    
    print(f"\n=== SCALING SYSTEM STATUS ===")
    print(f"Metrics collection active: {status['metrics_collection_active']}")
    print(f"Autoscaling active: {status['autoscaling_active']}")
    
    if status['latest_metrics']:
        metrics = status['latest_metrics']
        print(f"\nLatest Metrics:")
        print(f"  CPU: {metrics['cpu_percent']:.1f}%")
        print(f"  Memory: {metrics['memory_percent']:.1f}%")
        print(f"  Response Time: {metrics['response_time_ms']:.1f}ms")
        print(f"  Throughput: {metrics['throughput_rps']:.0f} RPS")
    
    scaling_status = status['scaling_status']
    print(f"\nScaling Status:")
    print(f"  Active Rules: {scaling_status['active_rules']}")
    print(f"  Current Instances: {scaling_status['current_instances']}")
    
    # Get capacity forecast
    forecast = orchestrator.capacity_planner.forecast_capacity_needs(30)
    
    if 'error' not in forecast:
        print(f"\n=== CAPACITY FORECAST (30 days) ===")
        forecasts = forecast['forecasts']
        print(f"CPU: {forecasts['cpu_percent']['current']:.1f}% -> {forecasts['cpu_percent']['forecasted']:.1f}%")
        print(f"Memory: {forecasts['memory_percent']['current']:.1f}% -> {forecasts['memory_percent']['forecasted']:.1f}%")
        
        print(f"\nRecommendations:")
        for rec in forecast['recommendations']:
            print(f"  - {rec['type'].upper()}: {rec['reason']}")
            print(f"    Action: {rec['recommended_action']}")
    
    # Cleanup
    orchestrator.shutdown_scaling_system()
    
    return orchestrator

# orchestrator = example_scaling_system()
```

## Scaling Best Practices

### Horizontal vs Vertical Scaling

1. **Horizontal Scaling (Scale Out)**
   - Add more instances to distribute load
   - Better fault tolerance and availability
   - Suitable for stateless applications
   - Linear cost scaling

2. **Vertical Scaling (Scale Up)**
   - Increase resources of existing instances
   - Simpler architecture and management
   - Better for stateful applications
   - Limited by hardware constraints

### Performance Optimization Strategies

1. **Caching Layers**
   - Implement multi-tier caching (L1, L2, L3)
   - Use Redis/Memcached for distributed caching
   - Cache frequently accessed data and queries
   - Implement cache warming and invalidation strategies

2. **Database Optimization**
   - Read replicas for read-heavy workloads
   - Database sharding for write scalability
   - Connection pooling and query optimization
   - Indexing strategies and query plan analysis

3. **Load Balancing**
   - Application-level load balancing
   - Database connection load balancing
   - Geographic load distribution
   - Health check and failover mechanisms

## Summary

Scaling Data Systems provides:

- **Automated Scaling** - Rule-based autoscaling with multiple triggers
- **Comprehensive Monitoring** - Real-time metrics collection and analysis
- **Capacity Planning** - Forecasting and proactive capacity management
- **Performance Optimization** - Multi-layer optimization strategies

Key components:
- Metrics collection and monitoring system
- Rule-based autoscaling engine
- Capacity forecasting and planning
- Performance optimization recommendations

---

**Next**: Learn about [Cost Optimization](/chapters/case-studies/cost-optimization) for managing data infrastructure costs effectively.
