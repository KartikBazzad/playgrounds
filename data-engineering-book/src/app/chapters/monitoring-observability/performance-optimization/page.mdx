import {MermaidDiagram} from '@/components/MermaidDiagram';

# Performance Optimization

Performance optimization in data engineering involves systematic analysis, monitoring, and tuning of data pipelines to achieve optimal throughput, latency, and resource utilization. This includes identifying bottlenecks, optimizing queries, and implementing efficient data processing patterns.

## Performance Optimization Architecture

<MermaidDiagram chart={`
graph TB
    subgraph "Performance Monitoring"
        PM1[Metrics Collection]
        PM2[Profiling Tools]
        PM3[Bottleneck Detection]
        PM4[Resource Monitoring]
    end
    
    subgraph "Analysis Engine"
        AE1[Performance Analysis]
        AE2[Query Optimization]
        AE3[Resource Analysis]
        AE4[Pattern Recognition]
    end
    
    subgraph "Optimization Strategies"
        OS1[Query Tuning]
        OS2[Index Optimization]
        OS3[Partitioning]
        OS4[Caching]
    end
    
    subgraph "Infrastructure Tuning"
        IT1[Resource Scaling]
        IT2[Memory Optimization]
        IT3[CPU Optimization]
        IT4[I/O Optimization]
    end
    
    PM1 --> AE1
    PM2 --> AE2
    PM3 --> AE3
    PM4 --> AE4
    
    AE1 --> OS1
    AE2 --> OS2
    AE3 --> OS3
    AE4 --> OS4
    
    OS1 --> IT1
    OS2 --> IT2
    OS3 --> IT3
    OS4 --> IT4
    
    style PM1 fill:#e3f2fd
    style AE1 fill:#e8f5e8
    style OS1 fill:#fff3e0
    style IT1 fill:#f3e5f5
`} />

## Performance Monitoring and Profiling

### Core Performance Framework

```python
# Performance Optimization System
import time
import psutil
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import statistics
import logging
from collections import defaultdict, deque
import pandas as pd
import numpy as np
from contextlib import contextmanager

class PerformanceMetricType(Enum):
    """Types of performance metrics"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    CPU_USAGE = "cpu_usage"
    MEMORY_USAGE = "memory_usage"
    DISK_IO = "disk_io"
    QUERY_TIME = "query_time"
    CACHE_HIT_RATE = "cache_hit_rate"

class OptimizationType(Enum):
    """Types of optimizations"""
    QUERY_OPTIMIZATION = "query_optimization"
    INDEX_OPTIMIZATION = "index_optimization"
    MEMORY_OPTIMIZATION = "memory_optimization"
    CACHE_OPTIMIZATION = "cache_optimization"
    PARTITIONING = "partitioning"
    PARALLELIZATION = "parallelization"

@dataclass
class PerformanceMetric:
    """Performance metric data point"""
    metric_type: PerformanceMetricType
    value: float
    timestamp: datetime
    context: Dict[str, Any] = field(default_factory=dict)
    tags: Dict[str, str] = field(default_factory=dict)

@dataclass
class PerformanceBottleneck:
    """Identified performance bottleneck"""
    bottleneck_id: str
    component: str
    metric_type: PerformanceMetricType
    severity: str  # low, medium, high, critical
    description: str
    impact_score: float
    detected_at: datetime
    suggested_optimizations: List[OptimizationType]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'bottleneck_id': self.bottleneck_id,
            'component': self.component,
            'metric_type': self.metric_type.value,
            'severity': self.severity,
            'description': self.description,
            'impact_score': self.impact_score,
            'detected_at': self.detected_at.isoformat(),
            'suggested_optimizations': [opt.value for opt in self.suggested_optimizations]
        }

class PerformanceProfiler:
    """Performance profiling and monitoring system"""
    
    def __init__(self, max_metrics: int = 10000):
        self.metrics: deque = deque(maxlen=max_metrics)
        self.baseline_metrics: Dict[str, List[float]] = defaultdict(list)
        self.logger = logging.getLogger(__name__)
    
    def record_metric(self, metric: PerformanceMetric):
        """Record a performance metric"""
        self.metrics.append(metric)
        
        # Update baseline for this metric type
        metric_key = f"{metric.metric_type.value}_{metric.tags.get('component', 'default')}"
        self.baseline_metrics[metric_key].append(metric.value)
        
        # Keep only recent baseline data
        if len(self.baseline_metrics[metric_key]) > 1000:
            self.baseline_metrics[metric_key] = self.baseline_metrics[metric_key][-1000:]
    
    @contextmanager
    def profile_operation(self, operation_name: str, component: str = "default", 
                         tags: Dict[str, str] = None):
        """Context manager for profiling operations"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            # Calculate metrics
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            
            # Record metrics
            base_tags = {'component': component, 'operation': operation_name}
            base_tags.update(tags or {})
            
            self.record_metric(PerformanceMetric(
                metric_type=PerformanceMetricType.LATENCY,
                value=duration,
                timestamp=datetime.now(),
                tags=base_tags
            ))
            
            self.record_metric(PerformanceMetric(
                metric_type=PerformanceMetricType.MEMORY_USAGE,
                value=memory_delta,
                timestamp=datetime.now(),
                tags=base_tags
            ))
            
            self.logger.info(f"Profiled {operation_name}: {duration:.3f}s, "
                           f"Memory: {memory_delta/1024/1024:.1f}MB")
    
    def get_performance_summary(self, component: str = None, 
                              hours: int = 24) -> Dict[str, Any]:
        """Get performance summary for specified time period"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # Filter metrics
        relevant_metrics = [
            m for m in self.metrics
            if m.timestamp >= cutoff_time and 
            (component is None or m.tags.get('component') == component)
        ]
        
        if not relevant_metrics:
            return {'message': 'No metrics found'}
        
        # Group by metric type
        metrics_by_type = defaultdict(list)
        for metric in relevant_metrics:
            metrics_by_type[metric.metric_type].append(metric.value)
        
        # Calculate statistics
        summary = {
            'component': component or 'all',
            'time_period_hours': hours,
            'total_metrics': len(relevant_metrics),
            'metric_statistics': {}
        }
        
        for metric_type, values in metrics_by_type.items():
            if values:
                summary['metric_statistics'][metric_type.value] = {
                    'count': len(values),
                    'mean': statistics.mean(values),
                    'median': statistics.median(values),
                    'min': min(values),
                    'max': max(values),
                    'p95': np.percentile(values, 95) if len(values) > 1 else values[0]
                }
        
        return summary

class BottleneckDetector:
    """Detects performance bottlenecks"""
    
    def __init__(self, profiler: PerformanceProfiler):
        self.profiler = profiler
        self.detected_bottlenecks: List[PerformanceBottleneck] = []
        self.logger = logging.getLogger(__name__)
    
    def detect_bottlenecks(self, component: str = None) -> List[PerformanceBottleneck]:
        """Run bottleneck detection"""
        new_bottlenecks = []
        
        # Detect high latency
        latency_bottlenecks = self._detect_high_latency(component)
        new_bottlenecks.extend(latency_bottlenecks)
        
        # Detect memory issues
        memory_bottlenecks = self._detect_memory_issues(component)
        new_bottlenecks.extend(memory_bottlenecks)
        
        # Detect throughput issues
        throughput_bottlenecks = self._detect_throughput_issues(component)
        new_bottlenecks.extend(throughput_bottlenecks)
        
        # Add to detected bottlenecks
        self.detected_bottlenecks.extend(new_bottlenecks)
        
        return new_bottlenecks
    
    def _detect_high_latency(self, component: str = None) -> List[PerformanceBottleneck]:
        """Detect high latency bottlenecks"""
        bottlenecks = []
        
        # Get recent latency metrics
        recent_metrics = [
            m for m in self.profiler.metrics
            if m.metric_type == PerformanceMetricType.LATENCY and
            m.timestamp >= datetime.now() - timedelta(minutes=30) and
            (component is None or m.tags.get('component') == component)
        ]
        
        if len(recent_metrics) < 10:
            return bottlenecks
        
        values = [m.value for m in recent_metrics]
        mean_latency = statistics.mean(values)
        p95_latency = np.percentile(values, 95)
        
        # Get baseline for comparison
        metric_key = f"latency_{component or 'default'}"
        baseline_values = self.profiler.baseline_metrics.get(metric_key, [])
        
        if baseline_values:
            baseline_mean = statistics.mean(baseline_values[-100:])
            
            # Check if current performance is significantly worse
            if mean_latency > baseline_mean * 1.5:  # 50% increase
                severity = "critical" if mean_latency > baseline_mean * 2 else "high"
                
                bottleneck = PerformanceBottleneck(
                    bottleneck_id=f"latency_{component or 'default'}_{int(time.time())}",
                    component=component or "default",
                    metric_type=PerformanceMetricType.LATENCY,
                    severity=severity,
                    description=f"High latency: {mean_latency:.3f}s (baseline: {baseline_mean:.3f}s)",
                    impact_score=mean_latency / baseline_mean,
                    detected_at=datetime.now(),
                    suggested_optimizations=[
                        OptimizationType.QUERY_OPTIMIZATION,
                        OptimizationType.INDEX_OPTIMIZATION,
                        OptimizationType.CACHE_OPTIMIZATION
                    ]
                )
                
                bottlenecks.append(bottleneck)
        
        return bottlenecks
    
    def _detect_memory_issues(self, component: str = None) -> List[PerformanceBottleneck]:
        """Detect memory-related bottlenecks"""
        bottlenecks = []
        
        recent_metrics = [
            m for m in self.profiler.metrics
            if m.metric_type == PerformanceMetricType.MEMORY_USAGE and
            m.timestamp >= datetime.now() - timedelta(hours=1) and
            (component is None or m.tags.get('component') == component)
        ]
        
        if len(recent_metrics) < 20:
            return bottlenecks
        
        values = [m.value for m in recent_metrics[-20:]]
        
        # Check for increasing memory trend
        if len(values) > 1:
            # Simple trend detection
            recent_avg = statistics.mean(values[-5:])
            older_avg = statistics.mean(values[:5])
            
            if recent_avg > older_avg * 1.5:  # 50% increase
                bottleneck = PerformanceBottleneck(
                    bottleneck_id=f"memory_{component or 'default'}_{int(time.time())}",
                    component=component or "default",
                    metric_type=PerformanceMetricType.MEMORY_USAGE,
                    severity="high",
                    description=f"Memory usage increasing: {recent_avg/1024/1024:.1f}MB recent vs {older_avg/1024/1024:.1f}MB older",
                    impact_score=recent_avg / older_avg,
                    detected_at=datetime.now(),
                    suggested_optimizations=[
                        OptimizationType.MEMORY_OPTIMIZATION,
                        OptimizationType.CACHE_OPTIMIZATION
                    ]
                )
                
                bottlenecks.append(bottleneck)
        
        return bottlenecks
    
    def _detect_throughput_issues(self, component: str = None) -> List[PerformanceBottleneck]:
        """Detect throughput degradation"""
        bottlenecks = []
        
        recent_metrics = [
            m for m in self.profiler.metrics
            if m.metric_type == PerformanceMetricType.THROUGHPUT and
            m.timestamp >= datetime.now() - timedelta(minutes=30) and
            (component is None or m.tags.get('component') == component)
        ]
        
        if len(recent_metrics) < 10:
            return bottlenecks
        
        values = [m.value for m in recent_metrics]
        current_throughput = statistics.mean(values[-5:])
        
        # Compare with baseline
        metric_key = f"throughput_{component or 'default'}"
        baseline_values = self.profiler.baseline_metrics.get(metric_key, [])
        
        if baseline_values:
            baseline_throughput = statistics.mean(baseline_values[-50:])
            
            if current_throughput < baseline_throughput * 0.7:  # 30% drop
                bottleneck = PerformanceBottleneck(
                    bottleneck_id=f"throughput_{component or 'default'}_{int(time.time())}",
                    component=component or "default",
                    metric_type=PerformanceMetricType.THROUGHPUT,
                    severity="high",
                    description=f"Throughput degradation: {current_throughput:.1f} vs baseline {baseline_throughput:.1f}",
                    impact_score=baseline_throughput / current_throughput,
                    detected_at=datetime.now(),
                    suggested_optimizations=[
                        OptimizationType.PARALLELIZATION,
                        OptimizationType.INDEX_OPTIMIZATION
                    ]
                )
                
                bottlenecks.append(bottleneck)
        
        return bottlenecks

class PerformanceOptimizer:
    """Performance optimization engine"""
    
    def __init__(self, profiler: PerformanceProfiler, detector: BottleneckDetector):
        self.profiler = profiler
        self.detector = detector
        self.optimization_history: List[Dict[str, Any]] = []
        self.logger = logging.getLogger(__name__)
    
    def analyze_and_optimize(self, component: str = None) -> Dict[str, Any]:
        """Analyze performance and suggest optimizations"""
        
        # Detect bottlenecks
        bottlenecks = self.detector.detect_bottlenecks(component)
        
        # Get performance summary
        performance_summary = self.profiler.get_performance_summary(component)
        
        # Generate optimization recommendations
        recommendations = self._generate_recommendations(bottlenecks)
        
        # Create analysis report
        analysis_report = {
            'analysis_timestamp': datetime.now().isoformat(),
            'component': component or 'all',
            'performance_summary': performance_summary,
            'bottlenecks_detected': len(bottlenecks),
            'bottlenecks': [b.to_dict() for b in bottlenecks],
            'optimization_recommendations': recommendations,
            'priority_actions': self._prioritize_actions(bottlenecks)
        }
        
        self.logger.info(f"Performance analysis completed: {len(bottlenecks)} bottlenecks detected")
        
        return analysis_report
    
    def _generate_recommendations(self, bottlenecks: List[PerformanceBottleneck]) -> List[Dict[str, Any]]:
        """Generate optimization recommendations"""
        recommendations = []
        
        # Group bottlenecks by optimization type
        optimization_groups = defaultdict(list)
        for bottleneck in bottlenecks:
            for opt_type in bottleneck.suggested_optimizations:
                optimization_groups[opt_type].append(bottleneck)
        
        # Generate recommendations
        for opt_type, related_bottlenecks in optimization_groups.items():
            impact_score = sum(b.impact_score for b in related_bottlenecks)
            
            recommendation = {
                'optimization_type': opt_type.value,
                'priority': impact_score * len(related_bottlenecks),
                'impact_score': impact_score,
                'affected_bottlenecks': len(related_bottlenecks),
                'description': self._get_optimization_description(opt_type),
                'implementation_steps': self._get_implementation_steps(opt_type),
                'estimated_effort': self._estimate_effort(opt_type)
            }
            
            recommendations.append(recommendation)
        
        # Sort by priority
        recommendations.sort(key=lambda x: x['priority'], reverse=True)
        
        return recommendations
    
    def _get_optimization_description(self, opt_type: OptimizationType) -> str:
        """Get description for optimization type"""
        descriptions = {
            OptimizationType.QUERY_OPTIMIZATION: "Optimize database queries and data processing logic",
            OptimizationType.INDEX_OPTIMIZATION: "Create or optimize database indexes",
            OptimizationType.MEMORY_OPTIMIZATION: "Optimize memory usage and reduce leaks",
            OptimizationType.CACHE_OPTIMIZATION: "Implement or optimize caching strategies",
            OptimizationType.PARTITIONING: "Implement data partitioning",
            OptimizationType.PARALLELIZATION: "Increase parallelization and concurrency"
        }
        return descriptions.get(opt_type, "General performance optimization")
    
    def _get_implementation_steps(self, opt_type: OptimizationType) -> List[str]:
        """Get implementation steps"""
        steps = {
            OptimizationType.QUERY_OPTIMIZATION: [
                "Analyze slow queries",
                "Optimize WHERE clauses",
                "Review join strategies",
                "Test query performance"
            ],
            OptimizationType.INDEX_OPTIMIZATION: [
                "Analyze query patterns",
                "Identify missing indexes",
                "Create appropriate indexes",
                "Monitor index usage"
            ],
            OptimizationType.MEMORY_OPTIMIZATION: [
                "Profile memory usage",
                "Identify memory leaks",
                "Optimize data structures",
                "Implement memory pooling"
            ]
        }
        return steps.get(opt_type, ["Analyze", "Plan", "Implement", "Test"])
    
    def _estimate_effort(self, opt_type: OptimizationType) -> str:
        """Estimate implementation effort"""
        effort_levels = {
            OptimizationType.QUERY_OPTIMIZATION: "Medium",
            OptimizationType.INDEX_OPTIMIZATION: "Low",
            OptimizationType.MEMORY_OPTIMIZATION: "High",
            OptimizationType.CACHE_OPTIMIZATION: "Medium",
            OptimizationType.PARTITIONING: "High",
            OptimizationType.PARALLELIZATION: "High"
        }
        return effort_levels.get(opt_type, "Medium")
    
    def _prioritize_actions(self, bottlenecks: List[PerformanceBottleneck]) -> List[Dict[str, Any]]:
        """Prioritize immediate actions"""
        critical_bottlenecks = [b for b in bottlenecks if b.severity == "critical"]
        high_bottlenecks = [b for b in bottlenecks if b.severity == "high"]
        
        priority_actions = []
        
        # Critical actions
        for bottleneck in critical_bottlenecks:
            priority_actions.append({
                'priority': 'CRITICAL',
                'action': f"Address {bottleneck.metric_type.value} bottleneck",
                'description': bottleneck.description,
                'component': bottleneck.component
            })
        
        # High priority actions
        for bottleneck in high_bottlenecks[:3]:
            priority_actions.append({
                'priority': 'HIGH',
                'action': f"Optimize {bottleneck.metric_type.value}",
                'description': bottleneck.description,
                'component': bottleneck.component
            })
        
        return priority_actions

# Example usage
def example_performance_optimization():
    """Example of performance optimization system"""
    
    # Initialize system
    profiler = PerformanceProfiler()
    detector = BottleneckDetector(profiler)
    optimizer = PerformanceOptimizer(profiler, detector)
    
    print("=== PERFORMANCE OPTIMIZATION SIMULATION ===")
    
    # Simulate operations with performance profiling
    components = ['data_ingestion', 'data_processing', 'data_storage']
    
    for i in range(20):
        component = components[i % len(components)]
        
        # Simulate operations with varying performance
        with profiler.profile_operation(f"process_batch_{i}", component):
            # Simulate performance degradation over time
            if i > 15:
                time.sleep(0.1 + (i - 15) * 0.02)  # Increasing latency
            else:
                time.sleep(0.05 + np.random.normal(0, 0.01))
        
        # Record throughput metrics
        throughput = 1000 / (0.05 + (0.05 if i > 15 else 0))
        profiler.record_metric(PerformanceMetric(
            metric_type=PerformanceMetricType.THROUGHPUT,
            value=throughput,
            timestamp=datetime.now(),
            tags={'component': component}
        ))
    
    # Run performance analysis
    print("\n=== PERFORMANCE ANALYSIS ===")
    
    for component in components:
        analysis = optimizer.analyze_and_optimize(component)
        
        print(f"\nComponent: {component}")
        print(f"  Bottlenecks detected: {analysis['bottlenecks_detected']}")
        print(f"  Recommendations: {len(analysis['optimization_recommendations'])}")
        print(f"  Priority actions: {len(analysis['priority_actions'])}")
        
        # Show top recommendation
        if analysis['optimization_recommendations']:
            top_rec = analysis['optimization_recommendations'][0]
            print(f"  Top recommendation: {top_rec['optimization_type']} "
                  f"(Priority: {top_rec['priority']:.1f})")
    
    # Overall summary
    overall_analysis = optimizer.analyze_and_optimize()
    print(f"\n=== OVERALL SUMMARY ===")
    print(f"Total bottlenecks: {overall_analysis['bottlenecks_detected']}")
    print(f"Critical actions needed: {len([a for a in overall_analysis['priority_actions'] if a['priority'] == 'CRITICAL'])}")
    
    return profiler, detector, optimizer

# profiler, detector, optimizer = example_performance_optimization()
```

## Best Practices

### Performance Optimization Guidelines

1. **Systematic Monitoring**
   - Implement comprehensive performance metrics
   - Establish baseline performance measurements
   - Monitor key performance indicators continuously
   - Set up automated bottleneck detection

2. **Data-Driven Optimization**
   - Use profiling data to identify bottlenecks
   - Prioritize optimizations by impact and effort
   - Measure performance before and after changes
   - Implement A/B testing for optimizations

3. **Query and Index Optimization**
   - Analyze query execution plans
   - Create appropriate indexes for frequent queries
   - Optimize JOIN operations and subqueries
   - Use query caching where beneficial

4. **Resource Management**
   - Monitor CPU, memory, and I/O utilization
   - Implement proper resource scaling strategies
   - Optimize memory allocation and garbage collection
   - Use connection pooling and resource pooling

## Summary

Performance Optimization provides:

- **Comprehensive Monitoring** - Detailed performance metrics and profiling
- **Bottleneck Detection** - Automated identification of performance issues
- **Optimization Recommendations** - Data-driven improvement suggestions
- **Priority-Based Actions** - Structured approach to performance improvements

Key components:
- Performance profiling and metrics collection
- Automated bottleneck detection algorithms
- Optimization recommendation engine
- Priority-based action planning

---

**Next**: Continue with [DevOps for Data Engineering](/chapters/devops) for comprehensive development and operations practices.
